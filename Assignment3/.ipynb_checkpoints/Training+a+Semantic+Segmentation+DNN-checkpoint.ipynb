{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Semantic Segmentation Network Ready for Deployment in the Car\n",
    "\n",
    "Lab created by Oliver Knieps\n",
    "\n",
    "---\n",
    "Before we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue.\n",
    "\n",
    "**NOTE: It is highly recommended to use Google Chrome as the web browser to run this lab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer should be three: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The answer should be three: \" + str(1+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  2 23:05:45 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 387.34                 Driver Version: 387.34                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   48C    P0    60W / 149W |     84MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Real-time scene understanding based upon camera images is a major part in nowadays’ perception modules for mobile robotics and self-driving cars. In the DNN domain, this can be solved by Fully Convolutional Networks (FCNs) that perform semantic segmentation in a single forward-pass [(Long et al., 2015)](#source_fcn). As the name suggests, a semantic segmentation FCN assigns each pixel of the input image to one of the predefined classes such as \"road\" or \"sky\": This output from the perception stage can effectively be fed into localization and path-planning modules. \n",
    "\n",
    "To give you a first impression of the task environment within this lab, look at the following frame and its pixel-wise, hand-labeled annotation from the Cityscapes dataset [(Cordts et al., 2016)](#source_cityscapes), a poplular Semantic Segmentation dataset that we will be using (this frame has been captured in Cologne, and you actually see the Cologne Cathedral on the horizon). For now, just think of every color in the image to the right as a different class (you may be able to guess the color-to-class mapping):\n",
    "\n",
    "<table><tr><td><img src='images/frame_0.png' width=\"480\"></td><td><img src='images/label_0.png' width=\"480\"></td></tr></table>\n",
    "This lab takes a classification network as a baseline and shows how to turn it into an FCN architecture that will be trained on a down-scaled subset of Cityscapes for several epochs. \n",
    "\n",
    "A Neural Network such as an FCN is nothing else than a long series of computations depending on each other from layer to layer – knowing a network's topology allows us to estimate how many operations are needed to perform one or more predictions. This information is very useful to meet practical requirements, given by e.g. the embedded platform we develop for, how many images we need to process in a specific use case, and/or what minimum detection performance is needed. In this lab, the target hardware will be DRIVE PX 2, and you will work on meeting the set of requirements for <b>Task 1</b> as outlined in the table below.  The other two task configurations are optional thought exercises.\n",
    "\n",
    "<a id='scenarios'></a>\n",
    "\n",
    "|Task number |Scenario|Number of classes|Number of cameras|Frame rate for processing|Required accuracy|Computational budget on platform|\n",
    "|-----|-----|-----|-----|-----|-----|-----|\n",
    "|1)|Parking scenario|7 classes: Sky, Infrastructure, Road, Sidewalk, Vehicles, Vulnerable Road Users, Void | 4 cameras| Medium frame rate: 15 fps | Medium accuracy|Low budget on platform (70 Giga operations per second)|\n",
    "|2)| Urban scenario|7 classes: Sky, Infrastructure, Road, Sidewalk, Vehicles, Vulnerable Road Users, Void | 4 cameras| High frame rate: 30 fps | Medium accuracy| High budget on platform (300 Giga operations per second |\n",
    "|3)| Rural scenario|2 classes: Road, Not Road | 1 camera | High frame rate: 30 fps | High accuracy | Medium budget on platform (120 Giga operations per second) |\n",
    "*Note: 1 Giga operation = $10^9$ operations. You will get an introduction on how we define \"accuracy\" in a Semantic Segmentation context later in this lab.*\n",
    "\n",
    "We will learn how to design DNNs for Semantic Segmentation with moderate accuracies on Cityscapes, and then develop an understanding of the available budget for deployment on the target platform.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Approach\n",
    "\n",
    "With this lab, we intend to shed light into creating a suitable FCN for a real-life Semantic Segmentation use case. To that end, we will modify an existing CNN architecture, MobileNets [(Howard et al., 2017)](#source_mobilenets), to perform pixel-wise classification in TensorFlow. We will familiarize ourselves with a down-scaled and simplified version of the Cityscapes dataset, which has a fully labeled training, validation and test set at an original color image resolution of 2048 by 1024 pixels. *Note that we will stick to the TensorFlow convention for dimensionality order of [height x width x channels], so that our original dataset's image size is [1024 x 2048 x 3].* In a \"real\" automotive scenario for Semantic Segmentation, images ingested into a DNN architecture are typically much larger than the one used in our toy example in order to have a better granularity of the predictions. This granularity is especially relevant when segmenting areas that are very distant from the camera. A small image size helps you speed up the CNN training process, so that you can spend more time on the important content of this lab.<br><br>\n",
    "\n",
    "\n",
    "<font color='blue'>**Important note about chapters marked blue:**<br>This is a lab at 'Advanced' level and presents many topics at a high level of detail, especially in the \"Theory and MobileNets\" chapters. Not all of this information is required to complete this lab; if some contents are totally new to you and you feel that you are stuck, don't skip but just skim through the corresponding chapters marked blue. There will be some time to re-read them during the training stage for your network. The same applies to blue exercises.</font>\n",
    "\n",
    "* [3. Theory](#chapter_theory): The next chapter covers the theoretical background needed for building a Fully Convolutional Network fitting into a previously known computational budget. The content of this chapter is targeted on a more proficient understanding and includes some questions. It first shows how to calculate the computational costs of a CNN in [3.1 Counting operations in a CNN](#subchapter_ops) (<font color='blue'>blue</font>), then introduces [3.2 Fully Convolutional Networks for Semantic Segmentation](#fcn_intro) (<font color='blue'>partially blue</font>) and eventually presents two metrics for assessing an FCN architecture's quality in [3.3 Measuring performance: Pixel-wise accuracy vs. Intersection over Union](#subchapter_perf) (<font color='blue'>partially blue</font>). \n",
    "* [4. MobileNets: An adjustable CNN as the fully convolutional stem](#chapter_mobilenets): You would rarely start from scratch when designing a Convolutional Neural Net for a specific purpose, and rather re-use successful architectures from academia and research. During this lab, we will make use of a subsets of the MobileNets topology. MobileNets, whose base version is designed for classification tasks, are interesting to us because they are generally not too resource-hungry and also give us a convenient parameter for scaling our computational needs – to the expense of a potentially lower performance. We will get to know MobileNets in general in [4.1 MobileNets basics](#subchapter_mobilenets_basics) (<font color='blue'>partially blue</font>), and then identify the subset which we can re-use for an FCN architecture in \n",
    "[4.2 Preparing MobileNets for a Semantic Segmentation task](#subchapter_mobilenets_prepare).\n",
    "* [5. Attaching a simple FCN head for Semantic Segmentation](#chapter_fcn_head): Having derived a subset of scalable MobileNets as our FCN \"stem\" and also having the background on FCN architectures from the Theory chapter, all we need to do now is to connect those dots. During the fifth chapter, we will turn FCN theory into practice and attach a simple FCN \"head\" to the MobileNets stem which allows us to train the resulting overall topology on a Semantic Segmentation task.\n",
    "* [6. Preparing the dataset and importing it into DIGITS](#chapter_dataset): We will train the FCN architecture generated in the preceding chapter in DIGITS, using its TensorFlow back-end. Before that, we will have to modify a down-sampled version of the Cityscapes dataset – reducing its labels to the number of classes given in your scenario – and then import it into DIGITS.\n",
    "* [7. Putting everything together: Training in DIGITS](#chapter_fcn_training): This is the final part of the lab where all the previously introduced parts come together. Having in mind [the specific use case you have been assigned to](#scenarios), you will first explore which MobileNets can potentially meet your scenario's budget constraints, attach an FCN head and train it on the previously modified Cityscapes dataset. Since we are collecting performance numbers from other participants at the end of this lab, you will get a feeling for the trade-off between resource consumption and performance in a CNN setting.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chapter_theory'></a>\n",
    "# 3. Theory\n",
    "<a id='subchapter_ops'></a>\n",
    "## <font color='blue'> 3.1 Counting operations in a CNN </font>\n",
    "State-of-the-art Convolutional Neural Networks require millions and billions of operations for a single prediction (even more for the back-propagation step during training). Since CNNs are that resource hungry and at the same time of paramount importance to nowadays' AI applications, they have become one of the favorite benchmarks for measuring a hardware's performance.\n",
    "\n",
    "The costliest as well as most frequent operation of CNNs – consuming easily more than 95% of the total computation time – is contained in their convolution layers (we will refer to this as \"Classical Convolution\", because you will get to know a less hungry convolution variant later). Let's understand how many operations Classical Convolution requires with the help of a simple example. It is not too important that you understand every detail of it, but it is key that you understand from which inputs the formula at the bottom of this paragraph can be derived:\n",
    "\n",
    "Consider you want to process an input color image with one red, one green and one blue channel ($c_i = 3$) at a height of $h_i = 4$ pixels and a width of $w_i = 8$ pixels through a Classical Convolution layer (without adding the bias term). The convolution shall use $n_f = 6$ square filters with a height of $h_f=5$ pixels and a width of $w_f=5$ pixels. We also do not want to skip any input pixel and set the stride to $s=1$. To facilitate the calculation a bit, let's assume that we will zero-pad our input image with $(5-1)/2 = 2$ pixels on all its borders, so that our output image has the same spatial dimensions as our input image: $h_o = h_i = 4$ and $w_o = w_i = 8$ (with a stride of 2, we would skip every other pixel and have a spatial output dimensionality of 2 by 4 pixels). Since each filter will create one output feature map of the previously mentioned spatial dimensions, we also know the output depth: $d_o = n_f = 6$. Here is a brief sketch of the input feature maps, one of the 6 filters/kernels, and the output feature maps:\n",
    "\n",
    "<img src='images/CNN_example.png'>\n",
    "\n",
    "That means every pixel, for example, the one marked with an 'X', of the 3-dimensional output volume with size ($h_o$ x $w_o$ x $d_o$) = ($4$ x $8$ x $6$) is the result of one convolution operation. Such operation is nothing else than multiplying $h_f * w_f * d_i = 5 * 5 * 3 = 75$ input pixels with the $h_f * w_f$ filter weights and accumulating them to a single value: That means it takes $h_f * w_f * d_i = 75$ multiplications and $h_f * w_f * d_i = 75$ additions, which yields $2 * h_f * w_f * d_i = 150$ operations per convolution. On a modern GPU, this can be performed especially efficiently because it can typically calculate one multiply-accumulate (MAC) operation per CUDA core (of which you have hundreds on the NVIDIA Pascal architecture) for float32 values (\"single precision\") in one cycle – sometimes even 2 or 4 when processing float16 (\"half precision\") or int8 values. The number of physical MAC units on a GPU available for operations at the target precision – determining how many MAC operations you can process per cycle – together with its clock frequency hence define its theoretical ability or budget to run CNN inference. As an embedded developer, you will often come to a situation where the budget for your application is just a fraction of the maximum budget. This is exactly what you see in the [use case you are working on!](#scenarios).\n",
    "\n",
    "To capture everything in one formula, it takes\n",
    "<a id='n_mac'></a>$$MAC^{classical} = h_o * w_o * n_f * h_f * w_f * c_i $$\n",
    "multiply-accumulates or\n",
    "$$ops^{classical} = 2 * MAC^{classical} = 2 * h_o * w_o * n_f * h_f * w_f * c_i $$\n",
    "\n",
    "operations per Classical Convolution layer, $h_o$ and $w_o$ being the output height and width of the layer, respectively, $n_f$ being the number of filters with a kernel height of $h_f$ and a kernel width of $w_f$, and $c_i$ being the number of input channels.\n",
    "<a id='q_3_1_1'></a>\n",
    "___\n",
    "### <font color='blue'>Question 3.1.1</font>\n",
    "How many operations does it take to compute above example using Classical Convolution?\n",
    "___\n",
    "Given the formula above, together with formulae to obtain the number of operations for less complex layer types, we can easily calculate the computational costs of popular CNN architectures. One common key performance indicator of nowadays' classification CNNs is the highest Top-1 accuracy achieved on the ImageNet [(Deng et al., 2009)](#source_imagenet) classification test dataset (next to the Top-5 accuracy). Plotting a neural network's computational needs for one prediction on the horizontal axis in Giga operations (note the logarithmic scale) and the Top-1 accuracy on the vertical axis, we see that high performing network topologies come with high costs; **the bubble size of each entry in the graph indicates how many parameters can be learned, which means that it shows a model's capacity**:\n",
    "<a id='cnn_graph'></a>\n",
    "<img src='images/CNN_gops.png'>\n",
    "*The Top-1 accuracies are based on data from https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models and https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md. The number of operations per prediction has been measured with TensorFlow implementations from https://github.com/tensorflow/models/tree/master/research/slim/nets.*\n",
    "\n",
    "It also becomes obvious that popular architectures such as AlexNet by [Krizhevsky et al. (2012)](#source_alexnet), (shown here is the a modified version accepting images of 224 by 224 pixels), GoogLeNet [(Szegedy et al., 2015)](#source_googlenet) and VGG-16 [(Simonyan and Zisserman, 2014)](#source_vgg) are not necessarily the best choice if your compute resources are constrained. In this lab, we will make use of the MobileNet architecture [(Howard et al., 2017)](#source_mobilenets) represented by the four leftmost data points. While one prediction with any of the four MobileNet versions requires fewer operations than when using AlexNet or GoogLeNet, the best performing MobileNet (MobileNet-1.0) also outperforms both of the latter in terms of Top-1 accuracy.\n",
    "<a id='q_3_1_2'></a>\n",
    "___\n",
    "### <font color='blue'>Question 3.1.2</font>\n",
    "Even though VGG-16 appears to be inferior to other topologies in terms of Top-1 classification accuracy and requires the most computations, what could be an argument of still using it? *Answer this question based only upon the graph.*\n",
    "\n",
    "___\n",
    "<a id='fcn_intro'></a>\n",
    "## 3.2 Fully Convolutional Networks for Semantic Segmentation\n",
    "\n",
    "Before introducing Fully Convolutional Networks, let's reiterate what we know about \"traditional\" CNN architectures used for compressing *all pixels* from the input image into *one vector* with as many one-dimensional elements as pre-defined classes, which allows us to determine a class belonging to the value with the highest score. Through a cascade of layers (e.g. convolution, activation, normalization, and pooling), spatial information from the input image is usually preserved until the first layer drastically reducing the spatial dimensionality of its input (for which the input has to be \"squeezed\"). This may be achieved through average pooling layers with large sliding windows for instance, or, occurring in nearly all popular classification CNNs, through one or more fully connected layers towards the end of the network topology.\n",
    "\n",
    "### 3.2.1. First observation: The task of classification kills spatial context at a certain point\n",
    "For AlexNet [(Krizhevsky et al., 2012)](#source_alexnet), the need for squeezing becomes obvious when looking at e.g. the MXNet visualization of its topology: In many Deep Learning frameworks, a fully connected layer can only take a vector as an input. For that reason, we first have to vectorize the output of the blue pooling layer `pool5` by feeding it through the orange `flatten0` layer, which is then connected to the fully connected layer`fc6`:\n",
    "<img src='images/AlexNet_head.png'>\n",
    "From a spatially ordered representation of $256$ feature maps of $6$ by $6$ pixels, we switch to an unordered representation of $256 * 6 * 6 = 9,216$ values. The following fully connected layer has exactly $9,216 * 4,096$ learnable weights (without the bias) and, unlike convolution layers, expects an input vector of exactly $9,216$ elements. That is also why we cannot easily modify the input size to a given classification topology. If we used a slightly larger input image for AlexNet that would result in an output of $256$ x $7$ x $7$ after the blue Pooling layer `pool5`, the output of the `Flatten` layer would be a $12,544$-element vector – which wouldn't fit the shape of the subsequent fully connected layer if it was trained with $9,216$ inputs.\n",
    "\n",
    "### 3.2.2 Second observation: Classification CNNs also \"classify\" pixel-wise\n",
    "\n",
    "Even for an off-the-shelf classification CNN pre-trained on e.g. the ImageNet [(Deng et al., 2009)](#source_imagenet) dataset, you will see that the feature maps created by convolutional layers just before being \"squeezed\" show interesting behavior if we use the left-hand side image from the Introduction as an input. To elaborate on this, we have fed that frame into an ImageNet pre-trained AlexNet and visualized its layer's activation in DIGITS.\n",
    "\n",
    "First of all, note that the images from the Cityscapes dataset all have non-squared dimensions, while AlexNet (the popular first version that won the ILSVRC-2012 competition) accepts color images of 227 by 227 pixels. Corresponding to a widely used convention, we fitted the original input into square dimensions and scaled it to AlexNet's expected input dimensions (also called \"squashing\") in the `data` layer:\n",
    "<img src='images/AlexNet_data.png'>\n",
    "Let's now jump to the feature maps produced by the network's last convolution layer (after ReLU activations), Layer `conv5`. \n",
    "<img src='images/AlexNet_conv5_box.png'>\n",
    "There we magnify one particular activation map marked green (second row, third column) from 13 by 13 pixels and overlay the squashed input image with it – low activations are shown in dark blue, medium activations in green, and high activations in red:\n",
    "<table><tr><td><img src='images/squashed.png' width=\"227\"></td><td><img src='images/overlay.png' width=\"227\"></td><td><img src='images/fill.png' width=\"227\"></td></tr></table>\n",
    "\n",
    "It looks like this highlighted feature map shows high activations in corresponding areas where the closest car is present (which wouldn't be too surprising, since the ImageNet dataset this AlexNet was trained with contains several car classes: http://image-net.org/search?q=car). Of course, this claim sounds a bit hand-wavy at this point since AlexNet had been trained for a different task, but think of this principle as a major motivation for Fully Convolutional Networks. This will be covered in the next section.\n",
    "\n",
    "<a id='fcn_details'></a>\n",
    "### 3.2.3 From one single prediction to several spatially ordered predictions through Fully Convolutional Networks \n",
    "\n",
    "In the example earlier, we found an interesting spatial relationship between the input image and one activation map after AlexNet's last convolution layer `conv5`. In fact, we could find that many neurons in the 256 feature maps created by that convolution layer \"fire\" in regions that may be of interest to Semantic Segmentation of our input image. What if we could replace the fully connected layers (for which we need to flatten spatially ordered feature maps) by one or several layers that classify pixel-wise and preserve locality? \n",
    "\n",
    "[As we remember](#fcn_intro), classification into $n$ classes through a CNN signifies creating one vector with $n$ elements per input to be classified. We have also learned that Semantic Segmentation actually means assigning one of $n$ classes to each pixel of an input image with the resolution $h_i$ x $w_i$. When using a CNN for a Semantic Segmentation task, the expected output of the topology should hence be a volume of the size $h_i$ x $w_i$ x $n$. We can achieve this by **convolutionizing** a CNN topology: Instead of using layers that destroy spatial context, we build in convolution layers, so that the CNN outputs a volume with a dimensionality of $h_i$ x $w_i$ x $n$. According to [Long et al. (2015)](#source_fcn), we will call such an architecture a **Fully Convolutional Network**.\n",
    "___\n",
    "*Fully Convolutional Networks put into context with the State of the Art*\n",
    "\n",
    "In practice, Fully Convolutional Networks often include so-called skip connections from feature maps of different resolution that are all upsampled to the target output size with separate Transposed Convolution layers, and eventually fused to a single output.\n",
    "\n",
    "Note, FCNs are not the only approach to solving Semantic Segmentation tasks through DNNs. For example, a pre-trained model of U-Net [(Ronneberger et al., 2015)](#source_unet) will be available in the [DIGITS 6 Model Store](#https://developer.nvidia.com/digits) soon. U-Net is a topology that first decreases the spatial input size with an “encoder” of Classical Convolution layers and then increases the resolution again to the input resolution, using a symmetrical “decoder” of Transposed Convolution layers.\n",
    "\n",
    "___\n",
    "*1x1 convolutions as pixel-wise classifiers*\n",
    "\n",
    "A fully connected layer is used for image classification since it has the ability to take all pixels in an input feature map into account for the prediction task. If we wanted to classify pixel-wise, what would we have to adjust to that approach? To get there, think of the following example where our input feature map's resolution is just 1x1 pixel ($h_i = w_i = 1$) with $c_i$ input channels, and we want to classify this huge \"column\" into $n$ classes in one layer.\n",
    "\n",
    "With a fully connected layer, each of the $c_i$ pixel slices would have a connection to each of the $n$ elements in the output vector; we would thus have $c_i * n$ learnable weights for these connections. \n",
    "\n",
    "Let's see what would happen if we used a Convolution layer with $n$ filters instead! Naturally, we would choose a filter height and width of 1 – a filter larger than the spatial input dimensions would not make much sense: $h_f = w_f = 1$. Since we are using $n$ filters, we would correspondingly create a volume at the same spatial dimensions as the input (1x1) and at a depth of $n$, i.e., another one-pixel column at the depth of $n$. Hence both input and output dimensions for the fully connected layer and the 1x1 (point-wise) convolution layer are the same; but what about the internal computations, are they also equivalent?\n",
    "\n",
    "Let's look at one of the $n$ output pixels generated from the 1x1 convolution layer: It is calculated from a point-wise multiplication of the weights from one of the filters with the input column of the same dimensionality, and  both the filter and the input have a dimensionality of [$1$ x $1$ x $c_i$]. This is nothing else than having $c_i$ direct connections with learnable weights from all input pixels to the output pixel in focus.  Now, knowing that there are $n$ output pixels, let's also connect our dots and derive the total number of learnable weights from such a 1x1 convolution layer – it is again $c_i * n$, just like in a fully connected layer, but we have the extremely beneficial property that we can use a convolutional layer flexibly with different spatial input sizes.\n",
    "\n",
    "Achieving an output volume of width and depth of $n$ filter maps seems to be relatively easy to achieve through a Classical Convolution layer: We can set the number of filters $n_f$ to $n$ and will end up with $n$ output feature maps – one for each class. The next section will show how a resolution of the input image, $h_i$ x $w_i$, can be met.\n",
    "\n",
    "This [video explanation](https://www.youtube.com/watch?v=9EZVpLTPGz8) by Andrew Ng can help further understand 1x1 convolutions. \n",
    "___\n",
    "<a id='transposed_conv'></a>\n",
    "*Transposed Convolution*\n",
    "\n",
    "Throughout this tutorial, we have only referred to convolution operations that either maintain the spatial input dimensions for the output feature maps (if the stride is $s=1$, and we have a \"full\" padding with $p=(f-1)/2$) or produce feature maps that have a lower resolution (e.g. for a stride $s>1$). So there is one question that remains: How can we \"upsample\" a highly condensed feature map with a resolution of just a few pixels to an original input resolution of e.g. 224 by 224 pixels? The answer is a layer type that you may have also heard of as \"Deconvolution\": Transposed Convolution. While the **Classical Convolution moves a filter window over an input volume** and calculates an output from a weighted sum of the pixels in that window, **Transposed Convolution moves a filter window over an output volume** that is initialized with zeros. It only takes a single input pixel, multiplies it with all filter weights, and adds the result to the corresponding elements in the output volume. It is also important to note that zero-padding for Transposed Convolution means adding additional pixels to the output feature map. \n",
    "\n",
    "Here a simple example taken from [the official Theano documentation](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#no-zero-padding-unit-strides-transposed):\n",
    "<img src='images/no_padding_no_strides.gif'>\n",
    "In above animation, we have an input of 2 x 2 pixels ($i = 2$) with just a single channel, and a Transposed Convolution kernel of 3 x 3 pixels ($k = 3$) – without any zero-padding so that $p=0$, and with a stride of $s=1$. Then we can calculate the output size with edge length $o$:\n",
    "\n",
    "$$o = s(i-1) + k - 2p$$\n",
    "\n",
    "Which, for the former example, would be $o = 1*(2-1)+3-2*0 = 4$. \n",
    "\n",
    "For Transposed Convolution, think of the sliding window as a brush tool as you know it from your favorite raster graphics editor such as GIMP: <br>\n",
    "In this virtual setting, our mouse pointer would determine the position of the input pixel. While you could control the size of the area that you \"paint\" color on through adjusting the kernel size $k$, the stride parameter $s$ would define the continuity of your brush strokes if you move your mouse pointer over the screen with the mouse button pressed. Within this analogy, the kernel weights would be a learnable shape and intensity of your brush area.\n",
    "\n",
    "*<font color='blue'>Calculating the parameters for Transposed Convolution</font>*\n",
    "\n",
    "To show you how to use this formula in practice, let's see how you would set the parameters to upscale an input size $i$, which may be the height or width, by a factor of $x$, so that $o = xi$:\n",
    "\n",
    "First of all, we would like to make sure that all our Transposed Convolution operations – or \"brush strokes\" – are overlapping. If we set the stride parameter $s$ to the same value as the kernel size $k$, we would not be able to achieve this because all window positions would be perfectly adjacent. However, if we choose the stride to be exactly half the kernel size so that $k = 2s$, we will always have an overlap of half a brush stroke between striding window positions. This sounds like a good setup, so let's keep this condition. \n",
    "\n",
    "Now we will apply a short-hand trick to achieve an output of $o = xi$ where we set $s=x$, $k=2s=2x$ and $p=s/2=x/2$ (if you are interested, you can look at how this setting has been derived after the lab):\n",
    "\n",
    "$$o = s(i-1) + k - 2p = x(i-1) + 2*x - 2(x/2) = x(i-1) + 2x-x = x(i-1) +x = xi$$\n",
    "\n",
    "In parameter space, we can thus determine a function for upscaling an input by a target factor $x$:\n",
    "$$\\theta(x): N_0 \\rightarrow N_0^3$$ \n",
    "$$\\theta(x) = (s,k,p)^T = (x,2x,x/2)^T,$$\n",
    "$s$ being the stride, $k$ the kernel size, and $p$ the zero padding parameters of the Transposed Convolution layer.\n",
    "<a id='transposed_question'></a>\n",
    "___\n",
    "### <font color='blue'>Question 3.2.3</font>\n",
    "Using above formula $\\theta(x)$, how would you set the parameters for stride $s$, kernel size $k$ and zero-padding $p$ if you wanted to upscale an image by the factor $x=32$?\n",
    "\n",
    "<a id='subchapter_perf'></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Questions 3.1.1 - 3.2.3\n",
    "\n",
    "3.1.1) How many operations does it take to compute the above example using Classical Convolution? \n",
    "\n",
    "####  See code below\n",
    "\n",
    "3.1.2) Even though VGG-16 appears to be inferior to other topologies in terms of Top-1 classification accuracy and requires the most computations, what could be an argument of still using it? Answer this question based only upon the graph.\n",
    "\n",
    "####  VGG-16 may still be favorable because of the large amounts of parameters, in other words, the model's capacity, it has the possibility of learning, relative to the other networks. \n",
    "\n",
    "3.2.3) Using above formula, how would you set the parameters for stride _s_, kernel size _k_ and zero-padding _p_ if you wanted to upscale an image by the factor  x = 32 ?\n",
    "\n",
    "#### theta(x) = (s,k,p).T = (32, 64, 16).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 28800 operations\n"
     ]
    }
   ],
   "source": [
    "ci = 3\n",
    "ho = 4\n",
    "wo = 8\n",
    "hf = 5\n",
    "wf = 5\n",
    "nf = 6\n",
    "\n",
    "def calcClassical(ho,wo,nf,hf,wf,ci):\n",
    "    MAC_classical = ho*wo*nf*hf*wf*ci \n",
    "    return 2*MAC_classical\n",
    "\n",
    "ops_classical = calcClassical(ho,wo,nf,hf,wf,ci)\n",
    "\n",
    "print('It would take {0} operations'.format(ops_classical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.3 Measuring performance: Pixel-wise accuracy vs. Intersection over Union\n",
    "\n",
    "\n",
    "### 3.3.1 Pixel-wise accuracy\n",
    "When classifying pixel-wise, you could treat each pixel prediction similarly to how you would for a \"traditional\" classification task. This would mean, if we had an image row of 1 x 4 pixels with the three leftmost pixels labeled as Class A and the rightmost pixel labeled as Class B, a single wrong prediction of the rightmost pixel would result in an accuracy of $3/4 = 75\\%$. \n",
    "\n",
    "<img src='images/accuracy_example.png' width=\"320\">\n",
    "\n",
    "In other words, if we predicted all 4 of the pixels to be class A, we would still achieve a relatively good accuracy of $75\\%$! Class imbalances are very common in Semantic Segmentation annotations: Think of an urban dataset taken from forward-looking camera mounted on a car where a huge portion of the pixels in *every* frame would be labeled as 'Sky', and comparably few pixels would be labeled as 'Pedestrian' in only those frames where pedestrians are visible. This is equivalent to our class A and class B example.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We hence need a stricter metric which is often preferred in a Semantic Segmentation task: Intersection over Union, which is used in the popular Pascal VOC Challenge ([(Everingham et al., 2010)](#source_iou), for instance.\n",
    "\n",
    "### <font color='blue'>3.3.2  Intersection over Union</font>\n",
    "\n",
    "While we can calculate the pixel-wise accuracy directly through comparison of the expected output (the label) and the predicted output of a pixel-wise classifier, Intersection over Union is first calculated for all classes separately, and then averaged over all classes for achieving the overall Intersection over Union score. This allows for a balanced analysis of all present classes; in practice, knowing that some classes are more relevant to the particular use case than others, weighted averages are also often used. In this lab, we will consider all classes as equally important, and just use the arithmetic mean over $N$ classes:\n",
    "\n",
    "$$IoU_{total} = \\frac{\\sum_{i=1}^{N}IoU^{class}_i}{N}$$\n",
    "\n",
    "Let's now explore how to calculate the class-wise Intersection over Union score $IoU^{class}:$\n",
    "Looking at prediction results for each class separately turns the problem into a binary classification with True Positives ($TP$), False Negatives($FN$), False Positives ($FP$) and True Negatives ($TN$). In case you do not remember how to determine these outcomes, we have listed them here for the class in focus:\n",
    "\n",
    "|Label|Prediction|Outcome|\n",
    "|-----|-----|-----|\n",
    "|Class in focus ($True$)|Class in focus($True$)|True Positive ($TP$)|\n",
    "|Class in focus ($True$)|Different class ($False$)|False Negative ($FN$)|\n",
    "|Different class ($False$)|Class in focus ($True$)|False Positive ($FP$)|\n",
    "|Different class ($False$)|Different class ($False$)|True Negative ($TN$)|\n",
    "\n",
    "For the class-wise Intersection over Union score $IoU^{class}$, we will not take into account any True Negatives ($TN$) – a value that is only dependent on predictions and labels for different classes than the one in focus:\n",
    "\n",
    "$$IoU^{class} = \\frac{\\sum{TP}}{\\sum{FN}+\\sum{FP}+\\sum{TP}}$$\n",
    "\n",
    "\n",
    "To extend our initial example with a 4-pixel image, we have calculated the class-wise IoU scores for both Class A and Class B below, and also the overall IoU metric (using the arithmetic mean). Those pixels that are used by the IoU formula are marked with a blue box:\n",
    "<img src='images/IoU_example.png' width=\"800\">\n",
    "As you see, a pixel-wise classifier only predicting Class A can more easily be interpreted as futile based upon the IoU score. We will accordingly use Intersection over Union as the key accuracy performance indicator during this lab, which is also the most common metric used for Semantic Segmentation competitions.\n",
    "\n",
    "### 3.3.3 Loss\n",
    "\n",
    "For the loss calculation, we will use an approach that is probably the most common for all kinds of classification DNNs: Cross Entropy Loss applied to class-wise probabilities for each pixel after using a Softmax operation. There are other NVIDIA Deep Learning Institute labs that pay special attention to different kinds of loss calculation for Semantic Segmentation, for example `Image Segmentation with TensorFlow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chapter_mobilenets'></a>\n",
    "# 4. MobileNets: An adjustable CNN as the fully convolutional stem\n",
    "\n",
    "Within the concept of Fully Convolutional Networks, we will use MobileNets [(Howard et al., 2017)](#source_mobilenets) as the encoder responsible for distilling a \"raw\" input image into a much lower resolution one, containing hundreds of learned features per pixel. \n",
    "\n",
    "<a id='subchapter_mobilenets_basics'></a>\n",
    "## 4.1 MobileNets basics\n",
    "\n",
    "Often design choices in the embedded domain are driven by computational requirements, [just like your target use case](#scenarios) – with MobileNets (notice the plural form: it is rather a set of networks than a single DNN), we are given a scalable concept for designing convolutional networks at different levels of fidelity, depending on the requirements of the task at hand.\n",
    "\n",
    "## <font color='blue'>4.1.1 Depth-wise Convolutions</font>\n",
    "Before looking at MobileNets' scalable nature, let's spend some time on understanding another key concept they make use of to keep the workload low and still achieve considerably good results: **Depth-wise Convolutions** [(Chollet, 2016)](#source_depthwise). Unlike the \"classical\" convolutions used in popular network topologies such as VGG [(Simonyan and Zisserman, 2014)](#source_vgg), GoogLeNet [(Szegedy et al., 2015)](#source_googlenet) or ResNet [(He et al., 2016)](#source_resnet), a single kernel from a Separable Convolution layer (e.g. at a size of $3$x$3$ pixels) convolves over all input feature maps separately in a first step – instead of convolving at the full depth of all input feature maps. This first step, also called Separable Convolutions, will always produce the same number of output feature maps as input feature maps and does not \"mix\" them; Separable Convolutions involve $MAC^{SC}$ multiply-accumulates:\n",
    "\n",
    "$$MAC^{SC} = h_o * w_o * c_i * h_f * w_f$$\n",
    "\n",
    "To overcome the deficiencies of not being able to choose the number of output feature maps and having no option for channel mixing, Separable Convolutions are followed by so-called Point-wise Convolutions (there are actually batch normalization and ReLU activation layers attached to them, but this does not affect the number of operations too much). This second step of Depth-wise Convolutions is simply a convolution layer with a spatial kernel size of 1x1 pixel, convolving over all \"stacked\" output feature maps of the Separable Convolution step. We can thus use the [the formula for calculating multiply-accumulates of Classical Convolution](#n_mac) from earlier, setting the kernel height and size to 1 so that our output resolution stays at $h_o$ x $w_o$, and keeping $c_i$ from the calculation of $MAC^{SC}$:\n",
    "\n",
    "$$MAC^{PW} = h_o * w_o * c_i * n_f * 1 * 1 = h_o * w_o * c_i * n_f$$\n",
    "\n",
    "\n",
    "Hence Depth-wise Convolutions require the following number of multiply-accumulates and operations:\n",
    "\n",
    "$$MAC^{DW} = MAC^{SC} + MAC^{PW} $$\n",
    "\n",
    "$$MAC^{DW} = h_o * w_o * c_i * h_f * w_f + h_o * w_o * c_i * n_f $$\n",
    "\n",
    "$$MAC^{DW} = h_o * w_o * c_i * (h_f * w_f + n_f) $$\n",
    "\n",
    "$$ops^{DW} = 2 * MAC^{DW} = 2 * h_o * w_o * c_i * (h_f * w_f + n_f) $$\n",
    "\n",
    "That means our reduction factor $r$ of operations for using Depth-wise Convolutions instead of Classical Convolution is:\n",
    "\n",
    "$$r_{ops} = \\frac{2 * h_o * w_o * c_i * h_f * w_f * n_f }{2 * h_o * w_o * c_i * (h_f * w_f + n_f)} = \\frac{h_f * w_f *n_f}{h_f * w_f + n_f}$$\n",
    "<a id='q_4_1_1'></a>\n",
    "___\n",
    "### <font color='blue'>Question 4.1.1</font>\n",
    "Let's consider the following:\n",
    "\n",
    "We have an input layer with [$h_i$ x $w_i$ x $c_i$] = [4 x 4 x 5] <br>\n",
    "We will use $n_f=8$ kernels with [$h_f$ x $w_f$ x $c_f$] = [3 x 3 x 5], and a padding of $p=1$ (`'SAME'` output resolution as input resolution) <br>\n",
    "\n",
    "What is the shape of the expected output assuming Classical Convolution?\n",
    "What is the reduction factor $r_{ops}$ of using Depth-wise Convolutions instead of Classical Convolution?\n",
    "___\n",
    "\n",
    "## 4.1.2 Depth Multiplier in MobileNets\n",
    "Using Separable Convolutions reduces the overall number of multiply accumulates for MobileNets, which helps to achieve an ImageNet top-1 classification accuracy slightly better than GoogLeNet at less than 40% of the floating-point operations (for MobileNet-**1.0**, note the factor at the end). But what if we have to use larger input images for recognizing a target level of detail – and this at frame rates of e.g. 30 fps – but MobileNet-1.0 does not fit into our available resource budget? \n",
    "\n",
    "MobileNets have specifically been designed to answer this type of question. At the expense of an expected lower accuracy (not only for the ImageNet classification challenge), we can adjust all convolutional layers' number of computations through a single parameter: The **Depth Multiplier**, which is appended to all MobileNet instances in  [the previously shown graph](#cnn_graph). The Depth Multiplier allows to decrease or increase the number of filters per Depth-wise Convolution layer according to your requirements; taking the example from earlier again as a baseline and setting the default depth to 1024, this is how the number of multiply-accumulates scales with different Depth Multipliers:\n",
    "\n",
    "|Depth Multiplier |MAC count of the example Depth-wise Convolution layer|\n",
    "|-----|-----|\n",
    "|8/1024 |4 x 4 x 5 x (3 x 3 + 8) = 1,360|\n",
    "|0.25 |4 x 4 x 5 x (3 x 3 + 256) = 21,200| \n",
    "|0.5 |4 x 4 x 5 x (3 x 3 + 512) = 41,680|\n",
    "|0.75 |4 x 4 x 5 x (3 x 3 + 768) = 62,160 |\n",
    "|1.0 |4 x 4 x 5 x (3 x 3 + 1024) = 82,640 |\n",
    "|1.25 |4 x 4 x 5 x (3 x 3 + 1280) = 103,120 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "That means, for large output depths, the number of MACs almost scales linearly with the Depth Multiplier since the filter size (here $3*3=9$) becomes negligible. Let's now use this knowledge in practice with the TensorFlow implementation of MobileNets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1.1\n",
    "\n",
    "What is the shape of the expected output assuming Classical Convolution? What is the reduction factor _rops_ of using Depth-wise Convolutions instead of Classical Convolution?\n",
    "\n",
    "#### See code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the expected output using classical convolution is [4x4x5]\n",
      "The reduction factor is equal to 4.235294117647059\n"
     ]
    }
   ],
   "source": [
    "ho = 4\n",
    "wo = 4\n",
    "ci = 5\n",
    "nf = 8\n",
    "hf = 3\n",
    "wf = 3\n",
    "cf = 5\n",
    "\n",
    "def calcDepthWise(ho,wo,nf,hf,wf,ci):\n",
    "    MAC_DW = ho*wo*ci*(hf*wf+nf)\n",
    "    return 2*MAC_DW\n",
    "\n",
    "r_ops = calcClassical(ho,wo,nf,hf,wf,ci)/(calcDepthWise(ho,wo,nf,hf,wf,ci))\n",
    "\n",
    "print('The shape of the expected output using classical convolution is [4x4x5]')\n",
    "print('The reduction factor is equal to {0}'.format(r_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subchapter_mobilenets_prepare'></a>\n",
    "## 4.2 Preparing MobileNets for a Semantic Segmentation task\n",
    "\n",
    "<img src=\"images/mobilenet_arch.png\">\n",
    "\n",
    "The above table describes MobileNet architecture as found on page 4 of the <a href=\"https://arxiv.org/abs/1704.04861\">paper</a>: *\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"* (authored by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam).\n",
    "\n",
    "The rest of MobileNet paper is optional reading material for your interests.  In the table above, we see a listing of layers and their respective input sizes in the leftmost column. What we really care about for convolutionalizing MobileNets is a layer's output size, which is unfortunately not explicitly listed: Here, an output size [$h_o$ x $w_o$ x $c_o$] at layer $n$ implies that the following layer $n+1$ has a corresponding input size. We can ignore the number of channels (the last factor in the rightmost column) for now, since we are primarily interested in the downsampling factor from layer to layer. For example, the first convolution layer \"Conv / s2\" (\"s2\" stands for a stride of two, skipping every other pixel during convolution) produces an output of half the spatial size of its input: [$112$ x $112$].\n",
    "\n",
    "We require a specific (fully convolutional) subset of MobileNets that we can re-use for our Semantic Segmentation task in mind. Luckily, the creators provide us with a handy function for constructing custom MobileNets. During this lab, not all of the optional arguments are of importance – we occluded the less relevant and focus on the following three: \n",
    "\n",
    "- Of course, it is necessary to specify a tensor containing the image data in **`inputs`** so that the spatial dimensions can iteratively be adjusted from layer to layer (you can specify tensors of input sizes other than [$224$ x $224$ x $3$])\n",
    "\n",
    "- The construction of our custom MobileNet instance will terminate at the layer with the string identifier **`final_endpoint`**. For instance, you could let your MobileNet creation stop after the first layer by filling in `'Conv2d_0'`, which is the only \"classical\" convolutional layer with kernel size $3$ x $3$ and a stride of $2$.\n",
    "\n",
    "- As described earlier, the parameter **`depth_multiplier`** dictates how many filters of the specified size are learned in **all convolutional layers**, and is therefore a useful measure to adjust the computational needs and the capacity of your network topology. By default, it is set to `1.0`. In the first layer `'Conv2d_0'`, this default setting results in $32$ filter maps. Lowering `depth_multiplier` to e.g. `0.5` would reduce the number of filter maps to $16$ for `'Conv2d_0'` – reducing the layer's computational requirements as well as its capacity – and correspondingly halve all other convolution layers' number of filter maps, too.\n",
    "\n",
    "Take a brief look at the reduced version of the function, and pay special attention to the options for `final_endpoint`:\n",
    "\n",
    "```python\n",
    "def mobilenet_v1_base(inputs,\n",
    "                      final_endpoint='TheLayerYouChoose',\n",
    "                      depth_multiplier=1.0)\n",
    "  \"\"\"Mobilenet v1.\n",
    "\n",
    "  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n",
    "\n",
    "  Args:\n",
    "    inputs: a tensor of shape [batch_size, height, width, channels].\n",
    "    final_endpoint: specifies the endpoint to construct the network up to. It\n",
    "      can be one of ['Conv2d_0', 'Conv2d_1_pointwise', 'Conv2d_2_pointwise',\n",
    "      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5'_pointwise,\n",
    "      'Conv2d_6_pointwise', 'Conv2d_7_pointwise', 'Conv2d_8_pointwise',\n",
    "      'Conv2d_9_pointwise', 'Conv2d_10_pointwise', 'Conv2d_11_pointwise',\n",
    "      'Conv2d_12_pointwise', 'Conv2d_13_pointwise'].\n",
    "    depth_multiplier: Float multiplier for the depth (number of channels)\n",
    "      for all convolution ops. The value must be greater than zero. Typical\n",
    "      usage will be to set this value in (0, 1) to reduce the number of\n",
    "      parameters or computation cost of the model.\n",
    "\n",
    "  Returns:\n",
    "    tensor_out: output tensor corresponding to the final_endpoint.\n",
    "    end_points: a set of activations for external use, for example summaries or\n",
    "                losses.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if final_endpoint is not set to one of the predefined values,\n",
    "                or depth_multiplier <= 0, or the target output_stride is not\n",
    "                allowed.\n",
    "  \"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id='exercise_A'></a>\n",
    "\n",
    "### Exercise A\n",
    "\n",
    "As mentioned before, MobileNet's strided convolution layers are responsible for gradually reducing the spatial input sizes to the network. Look at the table on page 4 of the MobileNet paper for answering the following questions:\n",
    "<a id='q_4_2_1a'></a>\n",
    "___\n",
    "#### Question 4.2.1\n",
    "Until which layer can we consider the network topology (with an input size of [$224$ x $224$ x $3$]) as a 'fully convolutional neural network', preserving spatial information from its input image?\n",
    "<a id='q_4_2_2a'></a>\n",
    "\n",
    "#### It appears that the network maintains its 'fully connected' status until the depthwise convolution with a stride of 2 (s=2) with the 3x3x256 dw filter. \n",
    "___\n",
    "#### Question 4.2.2\n",
    "In the largest fully convolutional subset of MobileNet, by which factor do we \"downsample\" the input image's spatial dimensions? Compare the input dimensions to the network and the output dimensions of the FCN subset.\n",
    "___\n",
    "\n",
    "#### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B\n",
    "Verify your assumptions from Exercise A by playing around with the following code snippet and answering the following questions:\n",
    "<a id='q_4_2_3b'></a>\n",
    "___\n",
    "#### Question 4.2.3\n",
    "If we want to encode the input image to a spatial representation at $\\frac{1}{32}$ of both the input height and the input width (from an input dimensionality of [$224$ x $224$ x $3$]), which layers can we select as an end point? Similarly, which layers can we choose as an endpoint if we want to \"downsample\" by a factor of 16? Modify `my_endpoint`.\n",
    "\n",
    "#### For a downsampling of 1/32, we can select the endpoint to be Conv2d_12_pointwise. For a downsampling of 1/16, we can select the endpoint to be Conv2d_11_pointwise. \n",
    "\n",
    "<a id='q_4_2_4b'></a>\n",
    "___\n",
    "#### Question 4.2.4\n",
    "If you were to change the input size to e.g. [$360$ x $480$ x $3$], do we achieve the same downsampling factor for input height and the width if we set `my_endpoint` to `'Conv2d_13_pointwise'`?\n",
    "\n",
    "#### Since the input image size is not square, the downsampling factor for height (1/30) is less than the downsampling factor width (1/32). \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: [1, 224, 224, 3]\n",
      "Output dimensions: [1, 112, 112, 32]\n",
      "Height downsampling factor: 2.0\n",
      "Width downsampling factor: 2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from mobilenet_v1 import *\n",
    "\n",
    "## Modify these lines for Question 4.2.4\n",
    "######################################################################\n",
    "height = 224 # The height of one input image\n",
    "width = 224 # The width of one input image\n",
    "######################################################################\n",
    "\n",
    "# We will replace this placeholder with our training images later\n",
    "my_image_tensor = tf.placeholder(tf.float32, shape=(1, height, width, 3))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Choose an appropriate endpoint for creating a custom MobileNet.\n",
    "# hint: Have a closer look at mobilenet_v1_base documentation above.\n",
    "my_endpoint = 'Conv2d_0'\n",
    "######################################################################\n",
    "\n",
    "# To prevent multiple graphs being active during one session, we reset the default graph every time we run this snippet\n",
    "tf.reset_default_graph()\n",
    "my_mobilenet,my_endpoints = mobilenet_v1_base(inputs=my_image_tensor,\n",
    "                  final_endpoint=my_endpoint,\n",
    "                  depth_multiplier=1.0)\n",
    "input_shape = my_image_tensor.get_shape().as_list()\n",
    "output_shape = my_mobilenet.get_shape().as_list()\n",
    "print('Input dimensions: ' + str(input_shape))\n",
    "print('Output dimensions: ' + str(output_shape))\n",
    "print('Height downsampling factor: %.1f' % (input_shape[1]/output_shape[1]))\n",
    "print('Width downsampling factor: %.1f' % (input_shape[2]/output_shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: [1, 360, 480, 3]\n",
      "Output dimensions: [1, 12, 15, 1024]\n",
      "Height downsampling factor: 30.0\n",
      "Width downsampling factor: 32.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from mobilenet_v1 import *\n",
    "\n",
    "## Modify these lines for Question 4.2.4\n",
    "######################################################################\n",
    "height = 360 # The height of one input image\n",
    "width = 480 # The width of one input image\n",
    "######################################################################\n",
    "\n",
    "# We will replace this placeholder with our training images later\n",
    "my_image_tensor = tf.placeholder(tf.float32, shape=(1, height, width, 3))\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Choose an appropriate endpoint for creating a custom MobileNet.\n",
    "# hint: Have a closer look at mobilenet_v1_base documentation above.\n",
    "my_endpoint = 'Conv2d_13_pointwise' \n",
    "######################################################################\n",
    "\n",
    "# To prevent multiple graphs being active during one session, we reset the default graph every time we run this snippet\n",
    "tf.reset_default_graph()\n",
    "my_mobilenet,my_endpoints = mobilenet_v1_base(inputs=my_image_tensor,\n",
    "                  final_endpoint=my_endpoint,\n",
    "                  depth_multiplier=1.0)\n",
    "input_shape = my_image_tensor.get_shape().as_list()\n",
    "output_shape = my_mobilenet.get_shape().as_list()\n",
    "print('Input dimensions: ' + str(input_shape))\n",
    "print('Output dimensions: ' + str(output_shape))\n",
    "print('Height downsampling factor: %.1f' % (input_shape[1]/output_shape[1]))\n",
    "print('Width downsampling factor: %.1f' % (input_shape[2]/output_shape[2]))\n",
    "\n",
    "#Conv2d_11_pointwise --> downsampling factor of 16\n",
    "#Conv2d_12_pointwise --> downsampling factor of 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id='chapter_fcn_head'></a>\n",
    "# 5. Attaching a simple FCN head for Semantic Segmentation\n",
    "\n",
    "We have derived a fully convolutional MobileNet stem whose computational complexity and capacity we can modify with the Depth Multiplier parameter. [As we had learned earlier](#fcn_details), this will not be sufficient to semantically segment an input image. We still require an \"appendix\" to the stem which classifies pixel-wise via (1x1) Point-wise Convolution and also up-samples to our original input image size through Transposed Convolution.\n",
    "\n",
    "# 5.1 A bilinear distribution for initializing Transposed Convolution kernels\n",
    "\n",
    "First of all let's elaborate on the layer type used for upsampling in an FCN that we learned about [in the Theory chapter](#transposed_conv): Transposed Convolution. As we remember, the kernel weights act as a kind of \"brush\" distributor of an input pixel's value, and we found a good setting for overlapping sliding window positions with \n",
    "$$\\theta(x) = (s,k,p)^T = (x,2x,x/2)^T,$$ $x$ being the factor we want to upsample by, and $s$, $k$, $p$ being the stride, kernel size, and padding parameters of the Transposed Convolution layer, respectively.\n",
    "\n",
    "According to the brush analogy, if we set all the weights within a Transposed Convolution kernel to the same value, we would achieve a drawing pattern similar to a marker pen: We would color all output pixels in the sliding window with the same intensity, which is derived from the corresponding input pixel. \n",
    "\n",
    "From the fully convolutional MobileNet base topology, we found out that we downsample by a factor of 32 horizontally and vertically. Correspondingly, in order to reach a classification granularity of one distinct class per pixel, we would have to eventually upsample by exactly the same factor. That means we can use the configuration for $s$, $k$ and $p$ that we [calculated earlier](#transposed_question).\n",
    "\n",
    "This implies that we have rather large square kernels with an edge length of $k=64$; instead of using a marker pen, we would actually prefer a \"real\" brush shape where we paint more color in central regions than in the surroundings. If you are working with image processing libraries such as OpenCV or OpenVX, this strategy is often referred to as upsampling with bilinear interpolation [(Prashanth et al., 2009)](#source_bilinear), and it can also be used for Fully Convolutional Networks [Long et al. (2015)](#source_fcn).\n",
    "\n",
    "Within the terminology of Transposed Convolution, we could interpret this as a setup where the pixels within a moving window on the output feature map being further away from the central input pixel contribute less to the final prediction than closer ones. \n",
    "<a id='exercise_C'></a>\n",
    "___\n",
    "### Exercise C\n",
    "___\n",
    "#### Question 5.1.1\n",
    "Linking all of the knowledge above together, we will use a bilinear distribution as the weights initializer for the Transposed Convolution kernels of our FCN architecture. The initial weights are created in the following code block; inspect the implementation, modify `nclasses` to the number of classes from your use case, modify `kernel_dims` to your calculated value from Question 3.2.3, and then run the cell to visualize your initialized weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAESCAYAAAArC7qtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef160daf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAEECAYAAACoUrZdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOk+VSqU6hdR5qlQq1Sn0/wFbS1TtXst0RgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef11face90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Adjust the following:\n",
    "######################################################################\n",
    "kernel_dims = [64,64]\n",
    "nclasses = 7\n",
    "######################################################################\n",
    "\n",
    "\n",
    "def create_bilinear_kernels(num_classes, kernel_dims):\n",
    "    \"\"\"Creates a 2D bilinear distribution for initializing the weights for transposed convolution.\"\"\"\n",
    "    \"\"\"Output: NumPy array with dimensions: [kernel_dims[0] x kernel_dims[1] x num_classes x num_classes]\"\"\"\n",
    "    def find_params(size):\n",
    "        \"\"\"For odd size: take the mid pixel as the center, for even size: take two adjacent pixels as the center.\"\"\"\n",
    "        divisor = np.floor((size + 1) / 2.0)\n",
    "        if size%2==0:\n",
    "            center = divisor - 0.5\n",
    "        else:\n",
    "            center = divisor - 1\n",
    "        return divisor,center\n",
    "\n",
    "    # note the expected shape of kernel_dims variable\n",
    "    height, width = kernel_dims\n",
    "    \n",
    "    # Determine the center and the divisor for height and width\n",
    "    divisor_height,center_height = find_params(height)\n",
    "    divisor_width,center_width = find_params(width)\n",
    "    \n",
    "    # Create one kernel with ones in the center, and decrease the value further away we move from there:\n",
    "    grid = np.ogrid[:height, :width]\n",
    "    filter_coefficients = (1.0 - abs(grid[0] - center_height) / divisor_height) * (1.0 - abs(grid[1] - center_width) / divisor_width)\n",
    "    # Now put everything into a NumPy array in the required float32 precision (shape: [height x width])\n",
    "    one_plane = np.array(filter_coefficients, dtype=np.float32)\n",
    "    # For each input feature map to each output feature map, \n",
    "    # we will use transposed convolution with a kernel of [height x width].\n",
    "    # There are num_classes input feature maps and also num_classes output feature maps.\n",
    "    # Hence we will first initialize a NumPy array with dimensionality [height x width x num_classes x num_classes]...\n",
    "    all_planes = np.zeros((height,\n",
    "                width,\n",
    "                num_classes,\n",
    "                num_classes), dtype=np.float32)\n",
    "\n",
    "    # ... and then copy one_plane to all num_classes x num_classes entries in the last two dimensions.\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            all_planes[:, :, i, j] = one_plane\n",
    "    return all_planes\n",
    "\n",
    "\n",
    "\n",
    "# Draw the weights for initializing one kernel\n",
    "bilinear_kernels = create_bilinear_kernels(nclasses, kernel_dims)\n",
    "plt.imshow(bilinear_kernels[:,:,0,0])\n",
    "plt.suptitle(\"One kernel with dimensions %d x %d\" %(kernel_dims[0],kernel_dims[1]), fontsize=15)\n",
    "plt.show() \n",
    "\n",
    "# Draw the miniatures of the weights for initializing all kernels\n",
    "fig, axes = plt.subplots(ncols=nclasses, nrows=nclasses)\n",
    "fig.suptitle((\"All %d x %d kernels\" %(nclasses,nclasses)), fontsize=15)\n",
    "for row in range(nclasses):\n",
    "    for col in range(nclasses):\n",
    "        axes[row, col].imshow(bilinear_kernels[:,:,row,col])\n",
    "        axes[row, col].get_xaxis().set_visible(False)\n",
    "        axes[row, col].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 A complete FCN head for Semantic Segmentation with a MobileNet stem\n",
    "\n",
    "In this section, we combine many learnings from earlier parts of this notebook. So read carefully and check for yourself if you have understood everything.\n",
    "\n",
    "We will construct a very simple Fully Convolutional Network [(Long et al., 2015)](#source_fcn) head to our MobileNet stem that enables training the overall architecture for a Semantic Segmentation task. Even though this FCN head is very basic – do not expect to win any competitions with it – it will do an adequate job on our Cityscapes dataset and can serve as solid learning material.\n",
    "\n",
    "Here is the architecture of mentioned FCN head that we will subsequently implement (note that we have omitted the activation and batch normalization layers for all convolution layers):\n",
    "\n",
    "<img src='images/FCN_head.png'>\n",
    "\n",
    "* As you can see, we append it to a pruned MobileNet with a spatial output dimensionality of 10 x 15, whereas the depth of the MobileNet output is determined by `1024*d_mul` (making use of the Depth Multiplier parameter, here `d_mul`, that we previously described). **In the code below, `d_mul` is referred to as `depth_multiplier`.**\n",
    "\n",
    "* The head's first convolution layer is there to replace the average pooling layer of the classification MobileNet topology with a window size of 7x7 (remember your findings from [Exercise A](#exercise_A)). For this layer, we have to choose a kernel size `k`, while we use TensorFlow's `'FULL'` padding policy and a stride of 1 so that our resolution does not change.  The greater the value for `k`, the greater the receptive field of the network, which basically represents the size of an area on the input image that is responsible for the prediction of one output pixel. To keep the computational needs within that layer low, we use the Depth-wise Convolution concept that is prominently present in MobileNets (if you are interested, you can calculate how many operations are required if we used Classical Convolution with a kernel size of `k=7` if you have 1024 input features and 1024 filters – as you may guess, a lot). Correspondingly, we have first have the Separable Convolution layer `conv1_separable`, producing `1024*d_mul` output feature maps, which is followed by a Point-wise Convolution layer, which we will describe in the following bullet point. **In the code below, `k` is referred to as `kernel`.**\n",
    "* The Point-wise Convolution layer `conv1_pointwise` has a 1x1 kernel and `d` filter maps, so that the output feature map depth is equal to `d`. **In the code below, `d` is referred to as `depth`.**\n",
    "* In the subsequent layer `conv2_classifier`, we intend to classify each of the 10x15 pixels received from the previous layer into `n` classes. We hence use another 1x1 convolution layer with `n` filters. **In the code below, `n` is referred to as `num_classes`.**\n",
    "* In principle, the output feature maps from the previous 1x1 convolution layer could already serve for a very coarse-grained Semantic Segmentation task where a region of 32x32 input pixels would be processed to a prediction of a single pixel. Our goal is to output predictions at a (single-)pixel granularity; we will therefore make use of the Transposed Convolution layer `upsampler` with the parameters [from the question you answered earlier](#transposed_question) and initialize the weights with the function we got to know in [Exercise C](#exercise_C). All the previous layers are initialized with the Xavier weight filler.\n",
    "___\n",
    "\n",
    "## Exercise D\n",
    "___\n",
    "\n",
    "### Question 5.2.1\n",
    "Now take a look at the following code block. Above layers' parameters can be adjusted in the function `add_fcn_head()`. To put this into the full context of our MobileNet architecture, `mobilenet_fcn()` creates the MobileNet stem we had derived earlier and attaches above FCN head to it. After studying the script below, fill in the number of classes you want to predict, and verify that you achieve a pixel-wise classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: [1, 320, 480, 3]\n",
      "Output dimensions: [1, 320, 480, 7]\n"
     ]
    }
   ],
   "source": [
    "def add_fcn_head(net, num_classes, input_shape, is_training, kernel=[7,7], depth=1024):\n",
    "    \"\"\"Adds a very simple fully convolutional head to the network specified with net.\"\"\"\n",
    "    \"\"\"num_classes defines the target output depth of the upsampling layer and the last point-wise convolution layer before that.\"\"\"\n",
    "    \"\"\"input_shape shape is needed for determining the target dimensionality after upsampling.\"\"\"\n",
    "    \"\"\"is_training indicates if we have to initialize the weights.\"\"\"\n",
    "    \"\"\"kernel determines the size of the first convolution layer's filter.\"\"\"\n",
    "    \"\"\"depth is the parameter for the number of the point-wise convolution layers' filters in this FCN head.\"\"\"\n",
    "    \n",
    "    # This is the Depth-wise Convolution block:\n",
    "    # First the Separable Convolution layer with the specified kernel, \n",
    "    # taking the output from the MobileNet stem as an input...\n",
    "    # Input shape: [10 x 15 x (1024*depth_multiplier from MobileNet stem)]\n",
    "    # Output shape: [10 x 15 x 1024*depth_multiplier from MobileNet stem)]\n",
    "    conv1_separable = slim.separable_conv2d(net, None, kernel,\n",
    "        depth_multiplier=1.0,\n",
    "        stride=[1,1],\n",
    "        normalizer_fn=slim.batch_norm,\n",
    "        scope='conv1_separable')\n",
    "    # ... and the following Point-wise Convolution layer.\n",
    "    # Input shape: [10 x 15 x (1024*depth_multiplier from MobileNet stem)]\n",
    "    # Output shape: [10 x 15 x depth]\n",
    "    conv1_pointwise = slim.conv2d(net, depth, [1, 1], 1, scope='conv1_pointwise')\n",
    "    \n",
    "    # Let's use Dropout layer with a common Dropout ratio of 0.5 for regularization.\n",
    "    # Input shape: [10 x 15 x depth]\n",
    "    # Output shape: [10 x 15 x depth]\n",
    "    dropout_keep_prob=0.5\n",
    "    dropout = slim.dropout(conv1_pointwise, dropout_keep_prob, is_training=is_training, scope='dropout')\n",
    "    \n",
    "    # The 1x1 Convolution layer, used as a classifier with num_classes output feature maps.\n",
    "    # Input shape: [10 x 15 x depth]\n",
    "    # Output shape: [10 x 15 x num_classes]\n",
    "    conv2_classifier = slim.conv2d(dropout, num_classes, [1, 1], 1, scope='conv2_classifier')\n",
    "    \n",
    "    # The Transposed Convolution Layer, upsampling from 10 x 15 to the original input image resolution of 320 x 480.\n",
    "    # Input shape: [10 x 15 x num_classes]\n",
    "    # Output shape: [320 x 480 x num_classes]\n",
    "    \n",
    "    # This is a fixed setting which can only upsample by a factor of 32.\n",
    "    # Let's first assert that this condition is met:\n",
    "    current_shape = net.get_shape().as_list()\n",
    "    factor_height = input_shape[1]/current_shape[1]\n",
    "    factor_width = input_shape[2]/current_shape[2]\n",
    "    upsampling_factor = 32\n",
    "    assert(factor_height==factor_width==upsampling_factor)\n",
    "    \n",
    "    # upsample_kernel_size = upsampling_factor * 2\n",
    "    upsample_kernel_size = 64\n",
    "    \n",
    "    if is_training:\n",
    "        # Only initialize the kernel weights with a bilinear distribution if we start from scratch during training.\n",
    "        filter_coefficients = create_bilinear_kernels(num_classes, [upsample_kernel_size, upsample_kernel_size])\n",
    "    else:\n",
    "        # Otherwise, just use zeros because we will reuse the pre-trained weights of a snapshot for inference.\n",
    "        filter_coefficients = [0]\n",
    "        \n",
    "    # upsample_stride = upsampling_factor\n",
    "    upsample_stride = 32\n",
    "    \n",
    "    upsampler = slim.conv2d_transpose(conv2_classifier, num_classes, \n",
    "        kernel_size = [upsample_kernel_size, upsample_kernel_size],\n",
    "        stride=[upsample_stride, upsample_stride],\n",
    "        scope='upsampler',\n",
    "        padding='SAME', # Note: With slim.conv2d_transpose, the padding will be calculated automatically.\n",
    "        weights_initializer=tf.constant_initializer(filter_coefficients),\n",
    "        trainable=True,\n",
    "        activation_fn=None) # Softmax will only be applied during training. For inference, we can use argmax.\n",
    "    return upsampler\n",
    "\n",
    "\n",
    "def mobilenet_fcn(input_tensor, num_classes, is_training=True, depth_multiplier=1.0, classifier_kernel=[7,7], classifier_depth=1024):\n",
    "    input_shape  = input_tensor.get_shape().as_list()\n",
    "    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        weights_regularizer=slim.l2_regularizer(1e-6),\n",
    "                        padding='SAME'):\n",
    "        base,_ = mobilenet_v1_base(input_tensor, scope='MobilenetV1', final_endpoint='Conv2d_13_pointwise', depth_multiplier=depth_multiplier)\n",
    "        fcn = add_fcn_head(base, num_classes, input_shape, is_training, kernel=classifier_kernel, depth=classifier_depth)\n",
    "        return fcn\n",
    "    \n",
    "    \n",
    "height_fcn = 320 # The height of one input image\n",
    "width_fcn = 480 # The width of one input image\n",
    "\n",
    "# Fill in the number of classes from your use case\n",
    "######################################################################\n",
    "number_classes_fcn = 7\n",
    "######################################################################\n",
    "\n",
    "\n",
    "my_image_tensor_fcn = tf.placeholder(tf.float32, shape=(1, height_fcn, width_fcn, 3))     \n",
    "\n",
    "tf.reset_default_graph()\n",
    "my_mobilenet_fcn = mobilenet_fcn(input_tensor=my_image_tensor_fcn, num_classes=number_classes_fcn)\n",
    "\n",
    "input_shape_fcn = my_image_tensor_fcn.get_shape().as_list()\n",
    "output_shape_fcn = my_mobilenet_fcn.get_shape().as_list()\n",
    "print('Input dimensions: ' + str(input_shape_fcn))\n",
    "print('Output dimensions: ' + str(output_shape_fcn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Question 5.2.2\n",
    "What are the two regularization methods that are applied in the code defining the network?  Type your answer in this Markdown cell.\n",
    "\n",
    "\n",
    "#### Regularization methods include dropout, and L2 regularization (slim.l2_regularizer). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id='chapter_dataset'></a>\n",
    "# 6. Preparing the dataset and importing it to DIGITS\n",
    "Originally, the Cityscapes dataset [(Cordts et al., 2016)](#source_cityscapes) actually contains images at a resolution of [$1024$ x $2048$] pixels with pixel-wise annotations for 30 classes, but we use a version downsized to [$320$ x $480$] in order to keep your training cycle times low. In the following, we will show you how we cropped and downs-sampled the images.\n",
    "\n",
    "As mentioned, the dimensions [$1024$ x $2048$] of our dataset are not really suitable for the given time frame within this lab. Additionally, the hood of the vehicle that Cityscapes has been recorded with is present at the bottom of all images. Also \"blurry\" borders can be seen on top of the images, which is the result of rectifying them according to the camera's lens properties after recording. We hence first cropped a central region from the original image and its corresponding annotation to the dimensions [$768$ x $1152$], and then downsampled it to [$320$ x $480$]. This process is shown with our example frame to give you an impression of the actual proportions we will be operating with:\n",
    "\n",
    "<img src='images/cropped_cityscapes.png'>\n",
    "\n",
    "By default, Cityscapes contains 3475 file pairs of camera images with their corresponding pixel-wise annotations (the 1525 test samples are unlabeled), out of which 2975 pairs belong to the training set taken in 18 German cities  (one of them being Cologne) and 500 to the validation set taken in Frankfurt, M&uuml;nster and Lindau. These original sets have been restructured as shown in the following table:\n",
    "\n",
    "|Original set |New set|Original #samples| New #samples|Comment|\n",
    "|-----|-----|-----|-----|-----|\n",
    "|Training set |Training set|2975|900|On average, drew  $900/2875 \\approx 30\\%$ from all 18 training cities (evenly distributed).|\n",
    "|Validation set (M&uuml;nster)|Validation set|174|100|Using 100 out of 174 samples from the validation set taken in M&uuml;nster as the new validation set (randomly sampled).|\n",
    "|Validation set (Lindau)|Test set |59 |59|Using all 59 images from the (fully labeled) validation set taken in Lindau as the new test set. The original Cityscapes test set does not contain labels.|\n",
    "\n",
    "We have ordered the pre-processed Cityscapes dataset in the following way: It consists of one features folder and one labels/annotations folder for each of the training, validation and test sets, totaling 6 folders.\n",
    "\n",
    "The features folders contain color images of size [$360$ x $480$ x $3$] with 8-bit RGB values. For example, a pixel  showing the sky in the upper central part of image `train/cologne_000012_000019.png` (the cropped one we see above) has the values `[194 210 210]`.\n",
    "\n",
    "The annotation folders contain pixel-wise label images of size [$360$ x $480$ x $1$] with 8-bit gray-scale values. The corresponding label for `train/cologne_000012_000019.png` is given the same file name, but logically resides in a different folder: `trainannot/cologne_000012_000019.png`. The pixel in the label image corresponding to the previously mentioned sky pixel is then containing the class index of \"Sky\": `23`.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Class reduction of Cityscapes\n",
    "\n",
    "Go through the following exercise and study the code block below.\n",
    "\n",
    "### Exercise E\n",
    "___\n",
    "#### Question 6.1.1\n",
    "First study, then modify the following code snippet to match your use case – pay special attention to the number of classes you want to detect, [as defined in your use case scenario](#scenarios). The output classes are predefined, however, you will have to figure out a mapping from the old classes to the 7 and 2 classes, respectively. Then run the script to crop all images to a resolution of 320 x 480 and perform the class re-mapping. The processed output files will be stored in `/data/Cityscapes_preprocessed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: Your class mapping configuration is corresponding to the expected number of classes.\n",
      "\n",
      "Started to process all images in /data/Cityscapes...\n",
      "No images to process.\n",
      "Started to process all images in /data/Cityscapes/test...\n",
      "Stored all processed images in /data/Cityscapes_preprocessed/test.\n",
      "Started to process all images in /data/Cityscapes/testannot...\n",
      "Stored all processed images in /data/Cityscapes_preprocessed/testannot.\n",
      "Started to process all images in /data/Cityscapes/val...\n",
      "Stored all processed images in /data/Cityscapes_preprocessed/val.\n",
      "Started to process all images in /data/Cityscapes/trainannot...\n",
      "Stored all processed images in /data/Cityscapes_preprocessed/trainannot.\n",
      "Started to process all images in /data/Cityscapes/valannot...\n",
      "Stored all processed images in /data/Cityscapes_preprocessed/valannot.\n",
      "Started to process all images in /data/Cityscapes/train...\n",
      "Stored all processed images in /data/Cityscapes_preprocessed/train.\n",
      "\n",
      "\n",
      "Done. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import distutils\n",
    "from distutils import dir_util\n",
    "from scipy import misc\n",
    "\n",
    "# Old classes:\n",
    "# 0: Unlabeled\n",
    "# 1: Ego vehicle\n",
    "# 2: Rectification border\n",
    "# 3: Out of roi\n",
    "# 4: Static\n",
    "# 5: Dynamic\n",
    "# 6: Ground\n",
    "# 7: Road\n",
    "# 8: Sidewalk\n",
    "# 9: Parking\n",
    "# 10: Rail track\n",
    "# 11: Building\n",
    "# 12: Wall\n",
    "# 13: Fence\n",
    "# 14: Guard rail\n",
    "# 15: Bridge\n",
    "# 16: Tunnel\n",
    "# 17: Pole\n",
    "# 18: Polegroup\n",
    "# 19: Traffic light\n",
    "# 20: Traffic sign\n",
    "# 21: Vegetation\n",
    "# 22: Terrain\n",
    "# 23: Sky\n",
    "# 24: Person\n",
    "# 25: Rider\n",
    "# 26: Car\n",
    "# 27: Truck\n",
    "# 28: Bus\n",
    "# 29: Caravan\n",
    "# 30: Trailer\n",
    "# 31: Train\n",
    "# 32: Motorcycle\n",
    "# 33: Bicycle\n",
    "\n",
    "\n",
    "# New class names for a class number of 7:\n",
    "# 0: Sky\n",
    "# 1: Infrastructure\n",
    "# 2: Road\n",
    "# 3: Sidewalk\n",
    "# 4: Vehicles\n",
    "# 5: VRU (Vulnerable Road Users)\n",
    "# 6: Void\n",
    "\n",
    "\n",
    "def reduce_classes(mapping_list, src_folder, dst_folder):\n",
    "    \"\"\"Walks iteratively through src_folder and crops all feature and label images to a height of 320 pixels.\"\"\"\n",
    "    \"\"\"Identifies label images if they have only one color channel and applies label conversion based on mapping_list.\"\"\"\n",
    "    \"\"\"Stores new files in dst_folder with the same folder topology as src_folder.\"\"\"\n",
    "    for root,dirs,files in os.walk(src_folder):\n",
    "        print('Started to process all images in %s...' % str(root))\n",
    "        dst_subfolder = os.path.join(dst_folder,root.split('/')[-1])\n",
    "        # Check if the current directory is a labels folder, if yes: perform label conversion...\n",
    "        if str(root).endswith('annot'):\n",
    "            for f in files:\n",
    "                if str(f).endswith('.png'):\n",
    "                    filename_src = os.path.join(root,f)\n",
    "                    image = misc.imread(filename_src)\n",
    "                    \n",
    "                    filename_dst = os.path.join(dst_subfolder,f)\n",
    "                    if not os.path.exists(dst_subfolder):\n",
    "                        os.makedirs(dst_subfolder)\n",
    "                    image_copy = image.copy()\n",
    "                    for mapping in mapping_list:\n",
    "                        image[image_copy==mapping.old_id] = mapping.new_id\n",
    "                    misc.imsave(filename_dst, image)\n",
    "            print('Stored all processed images in %s.' % str(dst_subfolder))\n",
    "        # ... if no, copy the current folder to the new directory if there are files present...\n",
    "        elif len(files) > 0:\n",
    "            distutils.dir_util.copy_tree(root, dst_subfolder)\n",
    "            print('Stored all processed images in %s.' % str(dst_subfolder))\n",
    "        # ... otherwise skip this folder (this will happen for the top folder /Cityscapes).\n",
    "        else:\n",
    "            print('No images to process.')\n",
    "    print('\\n\\nDone. ')\n",
    "                \n",
    "\n",
    "def check_mapping_list(mapping_list, number_classes):\n",
    "    \"\"\"This is a sanity check to make sure your mapping_list is aligned with the number of classes from your use case.\"\"\"\n",
    "    new_classes = [mapping.new_id for mapping in mapping_list]\n",
    "    unique_classes = set(new_classes)\n",
    "    if len(unique_classes) == number_classes:\n",
    "        print(\"PASS: Your class mapping configuration is corresponding to the expected number of classes.\\n\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"ERROR: Your class mapping configuration is NOT corresponding to the expected number of classes. Please check mapping_list and/or number_classes\")\n",
    "        return False\n",
    "\n",
    "\n",
    "class ClassMapping():\n",
    "    \"\"\"Simple data structure for the mapping from one original (old) class to one new class.\"\"\"\n",
    "    def __init__(self, old_id, new_id, old_name, new_name):\n",
    "        self.old_id = old_id\n",
    "        self.new_id = new_id\n",
    "        self.old_name = old_name\n",
    "        self.new_name = new_name\n",
    "\n",
    "# For sanity checking (a checksum), fill in the number of classes from your use case first:\n",
    "######################################################################\n",
    "number_classes = 7\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# How would you allocate the original classes to the target classes?\n",
    "# Using the example template provided below, complete mapping_list.\n",
    "######################################################################\n",
    "mapping_list=[\n",
    "    ClassMapping(old_id=0, new_id=6, old_name='Unlabeled', new_name='Void'),\n",
    "    ClassMapping(old_id=1, new_id=6, old_name='Ego vehicle', new_name='Void'),\n",
    "    ClassMapping(old_id=2, new_id=1, old_name='Rectification border', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=3, new_id=6, old_name='Out of roi', new_name='Void'),\n",
    "    ClassMapping(old_id=4, new_id=1, old_name='Static', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=5, new_id=5, old_name='Dynamic', new_name='VRU'),\n",
    "    ClassMapping(old_id=6, new_id=6, old_name='Ground', new_name='Void'),\n",
    "    ClassMapping(old_id=7, new_id=2, old_name='Road', new_name='Road'),\n",
    "    ClassMapping(old_id=8, new_id=3, old_name='Sidewalk', new_name='Sidewalk'),\n",
    "    ClassMapping(old_id=9, new_id=4, old_name='Parking', new_name='Vehicles'),\n",
    "    ClassMapping(old_id=10, new_id=1, old_name='Rail track', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=11, new_id=1, old_name='Building', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=12, new_id=1, old_name='Wall', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=13, new_id=1, old_name='Fence', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=14, new_id=1, old_name='Guard rail', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=15, new_id=1, old_name='Bridge', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=16, new_id=1, old_name='Tunnel', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=17, new_id=1, old_name='Pole', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=18, new_id=1, old_name='Polegroup', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=19, new_id=1, old_name='Traffic light', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=20, new_id=1, old_name='Traffic sign', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=21, new_id=6, old_name='Vegetation', new_name='Void'),\n",
    "    ClassMapping(old_id=22, new_id=1, old_name='Terrain', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=23, new_id=0, old_name='Sky', new_name='Sky'),\n",
    "    ClassMapping(old_id=24, new_id=5, old_name='Person', new_name='VRU'),\n",
    "    ClassMapping(old_id=25, new_id=5, old_name='Rider', new_name='VRU'),\n",
    "    ClassMapping(old_id=26, new_id=4, old_name='Car', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=27, new_id=4, old_name='Truck', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=28, new_id=4, old_name='Bus', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=29, new_id=4, old_name='Caravan', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=30, new_id=4, old_name='Trailer', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=31, new_id=4, old_name='Train', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=32, new_id=5, old_name='Motorcycle', new_name='VRU'),\n",
    "    ClassMapping(old_id=33, new_id=5, old_name='Bicycle', new_name='VRU'),\n",
    "]\n",
    "######################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Do not modify these two folders.\n",
    "src_folder = '/data/Cityscapes'\n",
    "dst_folder = '/data/Cityscapes_preprocessed'\n",
    "\n",
    "# Only run reduce_classes if the sanity check is successful.\n",
    "if(check_mapping_list(mapping_list, number_classes)):\n",
    "    reduce_classes(mapping_list, src_folder, dst_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Importing the modified Cityscapes dataset into DIGITS\n",
    "Now that we have the Cityscapes dataset in the resolution we require and have reduced the pixel-wise annotations to the number of classes specified in the scenario from the Introduction, we can go ahead and import everything into DIGITS. \n",
    "\n",
    "### <a href=\"/digits/\" target=\\\"_blank\\\">Open DIGITS</a> in a separate window first.\n",
    "In case you are new to DIGITS, take a brief look at [the overview given on the NVIDIA website](https://developer.nvidia.com/digits). Independent of the back-end framework you use DIGITS with (we will use TensorFlow), you may create datasets that can be interpreted by any of them and visualize the learning process as well as the inference results on selected input images in the same way.\n",
    "\n",
    "At first, click on the leftmost `Datasets` tab, then open the `New Datasets` drop-down menu to the right and select `Segmentation`.\n",
    "\n",
    "<img src='images/digits/digits_dataset_dropdown.png'>\n",
    "\n",
    "DIGITS is specifically designed for multiple users using the same hardware. Even though you will be the only user on the server instance that you run this Jupyter notebook on, create your personal user alias:\n",
    "\n",
    "<img src='images/digits/digits_username.png' width=\"800\">\n",
    "\n",
    "Now specify the paths to both the input images and the labels of your training and validation set by copying all settings shown in the screen shot below (do not forget to tick `Separate validation images`).\n",
    "\n",
    "In case you do not want to type the paths, here a table you can copy-paste from:\n",
    "\n",
    "|Field |Path|\n",
    "|-----|-----|\n",
    "|`Feature image folder` |`/data/Cityscapes_preprocessed/train`|\n",
    "|`Label image folder` |`/data/Cityscapes_preprocessed/trainannot`|\n",
    "|`Validation feature image folder` |`/data/Cityscapes_preprocessed/val`|\n",
    "|`Validation label image folder` |`/data/Cityscapes_preprocessed/valannot`|\n",
    "\n",
    "For `Class Labels`, choose the correct file corresponding to the number of classes from your use case:\n",
    "\n",
    "|Number of classes |Path for `Class Labels`|\n",
    "|-----|-----|\n",
    "|2 |`/data/label_lists/cityscapes_labels_2classes.txt`|\n",
    "|7 |`/data/label_lists/cityscapes_labels_7classes.txt`|\n",
    "\n",
    "\n",
    "<img src='images/digits/digits_dataset_settings1.png' width=\"800\">\n",
    "\n",
    "Now give your dataset a meaningful name (for example, `Cityscapes_2classes` or `Cityscapes_7classes`) and click `Create`:\n",
    "\n",
    "<img src='images/digits/digits_dataset_settings2.png' width=\"800\">\n",
    "\n",
    "It will take a short while until your dataset creation has finished. Once that happened, sanity-check the number of elements in your `train_db` and your `val_db`, together with their dimensionality. The result should look like this (with different path locations in the `/jobs` folder):\n",
    "\n",
    "<img src='images/digits/digits_dataset_sanity_check.png' width=\"640\">\n",
    "\n",
    "Now we are good to go and can proceed to the final section of this lab: The training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chapter_fcn_training'></a>\n",
    "# 7. Putting everything together: Training in DIGITS\n",
    "\n",
    "\n",
    "## 7.1 Exploring the parameter space for a suitable FCN architecture\n",
    "Before we start training, let's explore the available design space in terms of the four parameters we intend to modify:\n",
    "\n",
    "`DEPTH_MULTIPLIER`, which is the depth multiplier for your MobileNet stem. <br>\n",
    "`CLASSIFIER_KERNEL`, which is the kernel size for the Separable Convolution layer of the FCN head. <br>\n",
    "`CLASSIFIER_DEPTH`, which is the depth of the (first) Point-wise Convolution layer of the FCN head. <br>\n",
    "`NUMBER_CLASSES`, which is is the class number from your use case. <br>\n",
    "\n",
    "\n",
    "While `NUMBER_CLASSES` is predefined in [the use case scenario](#scenarios), we will have to modify the other 3 parameters to meet the most restrictive requirement: The available budget of Giga operations **per second**.\n",
    "\n",
    "**Per second** is an important indication here: In the Theory section, we learned how to calculate the number of operations for a single CNN forward pass at a given input size. But now we need to perform multiple predictions for potentially multiple cameras with number `n_cameras`, and this process is expected to be repeated at the frequency `fps_per_camera`. That means we have a total throughput of `images_per_second = n_cameras * fps_per_camera` images per second.\n",
    "\n",
    "If we do know the computational requirements for processing one image in Mega ($10^6$) operations with `megaops_per_image`, we can thus calculate the total requirements in Giga ($10^9$) operations per second:\n",
    "\n",
    "\n",
    "`gigaops_per_second = images_per_second * megaops_per_image / 1000`\n",
    "\n",
    "We now know the target output that should stay within our budget constraints, and also know which 3 parameters we can tune to reach that: In a first step, we would probably first choose a **`DEPTH_MULTIPLIER`** for defining the MobileNet stem: This will be the hungriest part of our Fully Convolutional Network.\n",
    "\n",
    "From the architecture of the FCN head, we see that the depth of the two final layers' depends on `NUMBER_CLASSES`, which we can directly fix according to the use case. The parameters **`CLASSIFIER_KERNEL`** and **`CLASSIFIER_DEPTH`** thus show the only potential for tuning the computational needs of the FCN head. Before we proceed to the final code blocks within this notebook:\n",
    "### Take a look at this spreadsheet on Google Drive: https://docs.google.com/spreadsheets/d/1cxCgM6XNZkF4Zj1jowypBk-LMau9MC2I2w0fHVjcWIM/edit?usp=sharing.\n",
    "\n",
    "On top, you see a table listing the number of Mega operations needed for a forward pass in the MobileNet stem, with different **`DEPTH_MULTIPLIER`** settings at an input size of [320 x 480 x 3].\n",
    "\n",
    "The lower two tables are correspondingly indicating the number of Mega operations for the FCN head, which is \n",
    "upsampling from [10 x 15 x (1024*`DEPTH_MULTIPLIER`)] to [320 x 480 x `NUMBER_CLASSES`] with values for `CLASSIFIER_KERNEL` and for `CLASSIFIER_DEPTH`.\n",
    "\n",
    "## Exercise F\n",
    "\n",
    "### Question 7.1.1\n",
    "Take pen and paper and estimate which parameter settings for  **`DEPTH_MULTIPLIER`**, **`CLASSIFIER_KERNEL`** and **`CLASSIFIER_DEPTH`** are candidates for meeting the budget requirements of your use case. Hint: The spreadsheet indicates numbers in Mega ($10^6$) operations per single prediction while your budget is given in Giga ($10^9$) operations per second, and you will have to make use of the formula for calculating `gigaops_per_second`. **It is recommended to use `DEPTH_MULTIPLIER` $\\in \\{0.25, 0.50, 0.75, 1.0\\}$**.\n",
    "\n",
    "#### TODO\n",
    "\n",
    "### Question 7.1.2\n",
    "\n",
    "Now verify your assumptions with below code block: From the previous question, you should know beforehand which `DEPTH_MULTIPLIER` is generally possible. Now you can play around with the other parameters, and also specify the number of cameras and the target frame rate from your use case. To get a better understanding about how the computations are distributed among the layers, we gave you a helper function printing out layer-wise statistics. If you have frozen your parameters, execute the code block and write down your computational needs in Giga operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensions: [1, 320, 480, 7]\n",
      "Layer name                                     MFLOPS      %\n",
      "-------------------------------------------  --------  -----\n",
      "MobilenetV1/Conv2d_0/convolution                 16.6    4.2\n",
      "MobilenetV1/Conv2d_1_depthwise/depthwise          5.5    1.4\n",
      "MobilenetV1/Conv2d_1_pointwise/convolution        9.8    2.5\n",
      "MobilenetV1/Conv2d_2_depthwise/depthwise          2.8    0.7\n",
      "MobilenetV1/Conv2d_2_pointwise/convolution        9.8    2.5\n",
      "MobilenetV1/Conv2d_3_depthwise/depthwise          5.5    1.4\n",
      "MobilenetV1/Conv2d_3_pointwise/convolution       19.7    5.0\n",
      "MobilenetV1/Conv2d_4_depthwise/depthwise          1.4    0.3\n",
      "MobilenetV1/Conv2d_4_pointwise/convolution        9.8    2.5\n",
      "MobilenetV1/Conv2d_5_depthwise/depthwise          2.8    0.7\n",
      "MobilenetV1/Conv2d_5_pointwise/convolution       19.7    5.0\n",
      "MobilenetV1/Conv2d_6_depthwise/depthwise          0.7    0.2\n",
      "MobilenetV1/Conv2d_6_pointwise/convolution        9.8    2.5\n",
      "MobilenetV1/Conv2d_7_depthwise/depthwise          1.4    0.3\n",
      "MobilenetV1/Conv2d_7_pointwise/convolution       19.7    5.0\n",
      "MobilenetV1/Conv2d_8_depthwise/depthwise          1.4    0.3\n",
      "MobilenetV1/Conv2d_8_pointwise/convolution       19.7    5.0\n",
      "MobilenetV1/Conv2d_9_depthwise/depthwise          1.4    0.3\n",
      "MobilenetV1/Conv2d_9_pointwise/convolution       19.7    5.0\n",
      "MobilenetV1/Conv2d_10_depthwise/depthwise         1.4    0.3\n",
      "MobilenetV1/Conv2d_10_pointwise/convolution      19.7    5.0\n",
      "MobilenetV1/Conv2d_11_depthwise/depthwise         1.4    0.3\n",
      "MobilenetV1/Conv2d_11_pointwise/convolution      19.7    5.0\n",
      "MobilenetV1/Conv2d_12_depthwise/depthwise         0.3    0.1\n",
      "MobilenetV1/Conv2d_12_pointwise/convolution       9.8    2.5\n",
      "MobilenetV1/Conv2d_13_depthwise/depthwise         0.7    0.2\n",
      "MobilenetV1/Conv2d_13_pointwise/convolution      19.7    5.0\n",
      "conv1_separable/depthwise                         3.8    1.0\n",
      "conv1_pointwise/convolution                      78.6   19.9\n",
      "conv1_pointwise/BiasAdd                           0.2    0.0\n",
      "conv2_classifier/convolution                      2.2    0.5\n",
      "conv2_classifier/BiasAdd                          0.0    0.0\n",
      "upsampler/conv2d_transpose                       60.2   15.2\n",
      "upsampler/BiasAdd                                 1.1    0.3\n",
      "Total                                           395.6  100.0\n",
      "47.5 Giga operations for 120 images per second\n"
     ]
    }
   ],
   "source": [
    "from calculate_ops import get_megaops\n",
    "\n",
    "# Modify these values:\n",
    "######################################################################\n",
    "DEPTH_MULTIPLIER= 0.25\n",
    "CLASSIFIER_KERNEL= [7,7]\n",
    "CLASSIFIER_DEPTH=1024\n",
    "NUMBER_CLASSES= 7\n",
    "######################################################################\n",
    "\n",
    "\n",
    "my_image_tensor_fcn = tf.placeholder(tf.float32, shape=(1, 320, 480, 3))\n",
    "tf.reset_default_graph()\n",
    "my_mobilenet_fcn = mobilenet_fcn(\n",
    "            input_tensor=my_image_tensor_fcn, \n",
    "            num_classes=NUMBER_CLASSES, \n",
    "            depth_multiplier=DEPTH_MULTIPLIER, \n",
    "            classifier_kernel=CLASSIFIER_KERNEL, \n",
    "            classifier_depth=CLASSIFIER_DEPTH\n",
    "        )\n",
    "\n",
    "output_shape_fcn = my_mobilenet_fcn.get_shape().as_list()\n",
    "print('Output dimensions: ' + str(output_shape_fcn))\n",
    "\n",
    "megaops_per_image = get_megaops(my_mobilenet_fcn, verbose=True)\n",
    "# Modify these values according to your use case:\n",
    "######################################################################\n",
    "n_cameras = 4\n",
    "fps_per_camera = 30\n",
    "######################################################################\n",
    "\n",
    "images_per_second = n_cameras * fps_per_camera\n",
    "\n",
    "gigaops_per_second = images_per_second * megaops_per_image / 1000\n",
    "\n",
    "print('%.1f Giga operations for %d images per second' % (gigaops_per_second, images_per_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Creating a MobileNet-FCN model in DIGITS\n",
    "\n",
    "Now we can finally make use of the dataset we previously created in DIGITS, and train our MobileNet-FCN architecture!\n",
    "\n",
    "### <a href=\"/digits/\" target=\\\"_blank\\\">Open DIGITS</a> in a separate window again.\n",
    "\n",
    "Similar to the dataset creation process, click on the `Models` tab, then open the `New Model` drop-down menu to the right and select `Segmentation`.\n",
    "\n",
    "<img src='images/digits/digits_model_dropdown.png'>\n",
    "\n",
    "As shown below, select your dataset in the top left box, and copy the following settings in the `Solver Settings` box. We will use the `Adam` solver [(Kingma and Ba, 2014)](#source_adam) with a `Fixed` (change to this mode from `Step` in the corresponding drop-down menu, which appears after checking the `Show advanced learning rate options` box) learning rate of `0.001`, the recommended setting from the classification MobileNet version we derived \"our\" MobileNet-FCN. In the `Data Transformations` box, select `Pixel`: This will subtract the same RGB value from all pixels for all input images. For now, we do not apply any data augmentations but you are invited to experiment with these settings outside of this lab:\n",
    "\n",
    "<img src='images/digits/digits_model_settings1.png' width=\"800\">\n",
    "\n",
    "Before we go further, prepare your clipboard with the following code; first fill in the values for `DEPTH_MULTIPLIER`, ` CLASSIFIER_KERNEL`, `CLASSIFIER_DEPTH`, `NUMBER_CLASSES` from the earlier code snippet that you feel comfortable with, and then **select ALL lines from code block below and copy them into your clipboard**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-40708d3b55be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named model"
     ]
    }
   ],
   "source": [
    "from model import Tower\n",
    "from utils import model_property\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import utils as digits\n",
    "import mobilenet_v1\n",
    "\n",
    "import sys\n",
    "sys.path.append('/notebooks')\n",
    "\n",
    "from mobilenet_fcn_from_notebook import add_fcn_head, iou_score\n",
    "\n",
    "#Copy the values from above into the following field, then copy all contents of this cell into your clipboard.\n",
    "######################################################################\n",
    "DEPTH_MULTIPLIER= 0.25\n",
    "CLASSIFIER_KERNEL= [7,7]\n",
    "CLASSIFIER_DEPTH=1024\n",
    "NUMBER_CLASSES= 7\n",
    "######################################################################\n",
    "\n",
    "def create_fcn_model(x, num_classes, is_training, depth_multiplier, kernel_classifier, depth_classifier):\n",
    "    input_shape  = x.get_shape().as_list()\n",
    "    net,_ = mobilenet_v1.mobilenet_v1_base(x, scope='MobilenetV1', final_endpoint='Conv2d_13_pointwise', depth_multiplier=depth_multiplier)\n",
    "    net = add_fcn_head(net, num_classes, input_shape, is_training, kernel=kernel_classifier, depth=depth_classifier)\n",
    "    return net\n",
    "\n",
    "class UserModel(Tower):\n",
    "\n",
    "    @model_property\n",
    "    def inference(self):\n",
    "        # The expected number of classes is defined first (we do not read this from the dataset):\n",
    "        self.nclasses = NUMBER_CLASSES\n",
    "        # The input batch has to be in the shape [batch_size x height x width x channels] (\"NHCW\"):\n",
    "        x = tf.reshape(self.x, shape=[-1, self.input_shape[0], self.input_shape[1], self.input_shape[2]])\n",
    "        # Use the following settings for layers of types slim.conv2d and slim.separable_conv2d\n",
    "        with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n",
    "                            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            weights_regularizer=slim.l2_regularizer(1e-6),\n",
    "                            padding='SAME'):\n",
    "            fcn = create_fcn_model(x, self.nclasses, self.is_training, DEPTH_MULTIPLIER, CLASSIFIER_KERNEL, CLASSIFIER_DEPTH)\n",
    "            if self.is_inference:\n",
    "                # If we use the network's prediction for visualization in DIGITS, let's convert the output from NHCW to NCHW.\n",
    "                fcn = digits.nhwc_to_nchw(fcn)\n",
    "        return fcn\n",
    "\n",
    "    @model_property\n",
    "    def loss(self):\n",
    "        predictions = self.inference # Load the inference network from above\n",
    "        labels = tf.to_int64(self.y) # Make sure that the labels are integers\n",
    "        # Reshape the predictions from [batch_size x 320 x 480 x 7] to a matrix of [(batch_size*320*480) x 7]:\n",
    "        predictions_reshaped = tf.reshape( predictions, [-1, self.nclasses] )\n",
    "        # Reshape the labels from [batch_size x 320 x 480] to a a vector with size [(batch_size*320*480)]:\n",
    "        labels_reshaped = tf.reshape( labels, [-1] )\n",
    "        # Let the reshaped predictions run through a Softmax layer, and then calculate the cross-entropy:\n",
    "        loss = digits.classification_loss(pred=predictions_reshaped, y=labels_reshaped)\n",
    "        # Calculate the Intersection over Union score. Ignore the last class ID and make it visible during training.\n",
    "        iou = iou_score(predictions_reshaped,labels_reshaped,skip_classes=[NUMBER_CLASSES-1])\n",
    "        self.summaries.append(tf.summary.scalar(iou.op.name, iou))\n",
    "        # Calculate the pixel-wise accuracy and make it visible during training.\n",
    "        accuracy = digits.classification_accuracy(pred=predictions_reshaped, y=labels_reshaped)\n",
    "        self.summaries.append(tf.summary.scalar(accuracy.op.name, accuracy))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Now scroll down, click on `Custom Network`, and then on `TensorFlow`. **Then paste the previously copied contents from your clipboard into the text box as shown below.**\n",
    "\n",
    "**If you are using Google Chrome (which is highly recommended), sanity-check if DIGITS can properly interpret your TensorFlow-based DNN by clicking on `Visualize Network`.**\n",
    "\n",
    "\n",
    "If you chose a MobileNet base architecture with a `DEPTH_MULTIPLIER` $\\in \\{0.25, 0.5, 0.75, 1.0\\}$, you can use the weights of a pre-trained classification for initialization; to do that, fill in the corresponding path in `Pretrained Model(s)`, as also shown in the image:\n",
    "\n",
    "|`DEPTH_MULTIPLIER` |Path in `Pretrained model(s)`|\n",
    "|-----|-----|\n",
    "|0.25 |`/data/mobilenet_snapshots/mobilenet_v1_0.25.ckpt`|\n",
    "|0.50 |`/data/mobilenet_snapshots/mobilenet_v1_0.50.ckpt`|\n",
    "|0.75 |`/data/mobilenet_snapshots/mobilenet_v1_0.75.ckpt`|\n",
    "|1.0 |`/data/mobilenet_snapshots/mobilenet_v1_1.0.ckpt`|\n",
    "\n",
    "\n",
    "<img src='images/digits/digits_model_settings2.png' width=\"800\">\n",
    "\n",
    "Per epoch, we will perform a forward pass for 1000 images (training + validation set combined), and perform a backward pass for the 900 training images. To save time, you could use several GPUs for distributed training. Let's first check how many GPUs we have at our exposure by running the following snippet (we could also look at `nvidia-smi`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available for training: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "local_devices = device_lib.list_local_devices()\n",
    "available_gpus =  sum(1 if x.device_type == 'GPU' else 0 for x in local_devices)\n",
    "\n",
    "print('Number of GPUs available for training: %d' % available_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following setting, you can distribute the training workload over your available GPUs: \n",
    "<img src='images/digits/digits_use_gpus.png' width=\"480\">\n",
    "\n",
    "We are finally there! Give your model a meaningful name and click on `Create` to start the training process. For 15 epochs running on a single GPU, this can easily take around 10 minutes (or even more):\n",
    "\n",
    "<img src='images/digits/digits_model_settings3.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Question 7.2.1\n",
    "How is accuracy (in training or validation) defined (as shown in your tensorflow board)? Why is accuracy of your net on this dataset much higher than the IoU?  Type your answers within this Markdown cell.\n",
    "\n",
    "#### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 7.3 Performance evaluation\n",
    "\n",
    "If you feel comfortable with your training results, the time for our final assessment has come: Let's visualize the trained MobileNet-FCN's inference on the Cityscapes test set in a video sequence and let's also find the Intersection over Union metric during that run. \n",
    "\n",
    "To do so, you will need to provide the job ID of your DIGITS training task. This is shown in the top left box of the model menu:\n",
    "<img src='images/digits/digits_job_dir.png' width=\"320\">\n",
    "Copy the job ID into your clipboard and paste it into the code block below. We gave you an example of the format below (to prevent that you confuse it with your own, the prefix dates our training period to the year 1900).\n",
    "\n",
    "In addition to the job ID, you can select the most promising snapshot epoch – typically the one scoring best for the validation score – and also fill that number into the code snippet below. To make sure that the target snapshot is really present (you could have specified earlier that you do not want to take a snapshot after every epoch), check the drop-down menu in the model menu, directly under your graphs:\n",
    "\n",
    "<img src='images/digits/digits_select_epoch.png' width=\"480\">\n",
    "\n",
    "At last, execute the cell. Look at the inference results, and write down the IoU score of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! All images have been evaluated. You can submit your score.\n",
      "IoU score for 59/59 images: 27.6\n"
     ]
    }
   ],
   "source": [
    "from run_inference_test import run_inference, get_labeled_pairs\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# Fill in the training job ID, together with the snapshot epoch that you would like to have assessed:\n",
    "######################################################################\n",
    "JOB_ID='20190202-230613-2391'\n",
    "SNAPSHOT_EPOCH = 15\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# Here we parse the test image folder and associate the correct label with each feature image.\n",
    "base_path = '/data/Cityscapes_preprocessed/'\n",
    "image_folder = base_path + 'test'\n",
    "label_folder = base_path + 'testannot'\n",
    "filename_pairs = get_labeled_pairs(image_folder,label_folder)\n",
    "\n",
    "color_map = [\n",
    "    (70, 130, 180), # 0: Sky - light blue\n",
    "    (255, 255, 0), # 1: Infrastructure - yellow\n",
    "    (0, 255, 0), # 2: Road - green\n",
    "    (244, 35, 232), # 3: Sidewalk - purple\n",
    "    (0, 0, 255), # 4: Vehicles - dark blue\n",
    "    (255, 0, 0), # 5: VRU (Vulnerable Road Users) - red\n",
    "    (0, 0, 0) # 6: Void - transparent\n",
    "]\n",
    "\n",
    "job_path = '/jobs/' + JOB_ID\n",
    "run_inference(job_path, SNAPSHOT_EPOCH, filename_pairs, color_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Question 7.3.1\n",
    "According to latest research, what change in the architecture of the stem of your network could provide an additional boost in\t IoU? Type your answers within this Markdown cell. Hint: recall our discussion of latest papers in segmentation in class and think about an architecture that would feed both details and context for details into the FCN head.\n",
    "\n",
    "#### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Sources\n",
    "\n",
    "<a id='source_fcn'></a>\n",
    "Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic\n",
    "segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
    "Recognition (pp. 3431-3440).\n",
    "\n",
    "<a id='source_cityscapes'></a>\n",
    "Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., ... & Schiele, B. (2016). The Cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3213-3223).\n",
    "\n",
    "<a id='source_mobilenets'></a>\n",
    "Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., ... & Adam, H. (2017). Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.\n",
    "\n",
    "<a id='source_imagenet'></a>\n",
    "Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 248-255). IEEE.\n",
    "\n",
    "<a id='source_alexnet'></a>\n",
    "Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep\n",
    "convolutional neural networks. In Advances in neural information processing systems (pp.\n",
    "1097-1105).\n",
    "\n",
    "<a id='source_googlenet'></a>\n",
    "Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A.\n",
    "(2015). Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer\n",
    "Vision and Pattern Recognition (pp. 1-9).\n",
    "\n",
    "<a id='source_vgg'></a>\n",
    "Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n",
    "\n",
    "<a id='source_unet'></a>\n",
    "Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 234-241). Springer, Cham.\n",
    "\n",
    "<a id='source_iou'></a>\n",
    "Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisserman, A. (2010). The pascal visual object classes (VOC) challenge. International journal of computer vision, 88(2), 303-338.\n",
    "\n",
    "<a id='source_bilinear'></a>\n",
    "Prashanth, H. S., Shashidhara, H. L., & KN, B. M. (2009). Image scaling comparison using universal image quality index. In Advances in Computing, Control, & Telecommunication Technologies, 2009. ACT'09. International Conference on (pp. 859-863). IEEE.\n",
    "\n",
    "<a id='source_depthwise'></a>\n",
    "Chollet, F. (2016). Xception: Deep Learning with Depthwise Separable Convolutions. arXiv preprint arXiv:1610.02357.\n",
    "\n",
    "<a id='source_resnet'></a> \n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition.\n",
    "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 770-\n",
    "778).\n",
    "\n",
    "<a id='source_adam'></a>\n",
    "Kingma, D., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\n",
    "arXiv:1412.6980."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
