{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Answers to A3 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1.1\n",
    "\n",
    "### How many operations does it take to compute the above example using Classical Convolution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 28800 operations\n"
     ]
    }
   ],
   "source": [
    "ci = 3\n",
    "ho = 4\n",
    "wo = 8\n",
    "hf = 5\n",
    "wf = 5\n",
    "nf = 6\n",
    "\n",
    "def calcClassical(ho,wo,nf,hf,wf,ci):\n",
    "    MAC_classical = ho*wo*nf*hf*wf*ci \n",
    "    return 2*MAC_classical\n",
    "\n",
    "ops_classical = calcClassical(ho,wo,nf,hf,wf,ci)\n",
    "\n",
    "print('It would take {0} operations'.format(ops_classical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1.2\n",
    "\n",
    "### Even though VGG-16 appears to be inferior to other topologies in terms of Top-1 classification accuracy and requires the most computations, what could be an argument of still using it? Answer this question based only upon the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG-16 may still be favorable because of the large amounts of parameters, in other words, the model's capacity, it has the possibility of learning, relative to the other networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2.3 \n",
    "\n",
    "### Using above formula, how would you set the parameters for stride _s_, kernel size _k_ and zero-padding _p_ if you wanted to upscale an image by the factor  x = 32 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### theta(x) = (s,k,p).T = (32, 64, 16).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1.1\n",
    "\n",
    "### What is the shape of the expected output assuming Classical Convolution? What is the reduction factor _rops_ of using Depth-wise Convolutions instead of Classical Convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the expected output using classical convolution is [4x4x5]\n",
      "The reduction factor is equal to 4.235294117647059\n"
     ]
    }
   ],
   "source": [
    "ho = 4\n",
    "wo = 4\n",
    "ci = 5\n",
    "nf = 8\n",
    "hf = 3\n",
    "wf = 3\n",
    "cf = 5\n",
    "\n",
    "def calcDepthWise(ho,wo,nf,hf,wf,ci):\n",
    "    MAC_DW = ho*wo*ci*(hf*wf+nf)\n",
    "    return 2*MAC_DW\n",
    "\n",
    "r_ops = calcClassical(ho,wo,nf,hf,wf,ci)/calcDepthWise(ho,wo,nf,hf,wf,ci)\n",
    "\n",
    "print('The shape of the expected output using classical convolution is [4x4x5]')\n",
    "print('The reduction factor is equal to {0}'.format(r_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.1 - use following code to test\n",
    "\n",
    "### Until which layer can we consider the network topology (with an input size of [ 224 x 224 x 3 ]) as a 'fully convolutional neural network', preserving spatial information from its input image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.2\n",
    "\n",
    "### In the largest fully convolutional subset of MobileNet, by which factor do we \"downsample\" the input image's spatial dimensions? Compare the input dimensions to the network and the output dimensions of the FCN subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.3\n",
    "\n",
    "### If we want to encode the input image to a spatial representation at 1/32 of both the input height and the input width (from an input dimensionality of [ 224 x 224 x 3 ]) , which layers can we select as an end point? Similarly, which layers can we choose as an endpoint if we want to \"downsample\" by a factor of 16? Modify my_endpoint .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.4\n",
    "\n",
    "### If you were to change the input size to e.g. [360 x 480 x 3], do we achieve the same downsampling factor for input height and the width if we set my_endpoint to 'Conv2d_13_pointwise' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-2f837c954c36>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-2f837c954c36>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from tensorflow.contrib import slim from mobilenet_v1 import *\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim from mobilenet_v1 import *\n",
    "\n",
    "## Modify these lines for Question 4.2.4 \n",
    "###################################################################### \n",
    "height = 224 # The height of one input image\n",
    "width = 224 # The width of one input image \n",
    "######################################################################\n",
    "\n",
    "# We will replace this placeholder with our training images later\n",
    "my_image_tensor = tf.placeholder(tf.float32, shape=(1, height, width, 3))\n",
    "\n",
    "\n",
    "#### GET REST OF CODE FROM NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
