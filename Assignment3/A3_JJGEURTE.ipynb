{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to A3 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1.1\n",
    "\n",
    "How many operations does it take to compute the above example using Classical Convolution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 28800 operations\n"
     ]
    }
   ],
   "source": [
    "ci = 3\n",
    "ho = 4\n",
    "wo = 8\n",
    "hf = 5\n",
    "wf = 5\n",
    "nf = 6\n",
    "\n",
    "def calcClassical(ho,wo,nf,hf,wf,ci):\n",
    "    MAC_classical = ho*wo*nf*hf*wf*ci \n",
    "    return 2*MAC_classical\n",
    "\n",
    "ops_classical = calcClassical(ho,wo,nf,hf,wf,ci)\n",
    "\n",
    "print('It would take {0} operations'.format(ops_classical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1.2\n",
    "\n",
    "Even though VGG-16 appears to be inferior to other topologies in terms of Top-1 classification accuracy and requires the most computations, what could be an argument of still using it? Answer this question based only upon the graph.\n",
    "\n",
    "#### VGG-16 may still be favorable because of the large amounts of parameters, in other words, the model's capacity, it has the possibility of learning, relative to the other networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2.3 \n",
    "\n",
    "Using above formula, how would you set the parameters for stride _s_, kernel size _k_ and zero-padding _p_ if you wanted to upscale an image by the factor  x = 32 ?\n",
    "\n",
    "#### theta(x) = (s,k,p).T = (32, 64, 16).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1.1\n",
    "\n",
    "What is the shape of the expected output assuming Classical Convolution? What is the reduction factor _rops_ of using Depth-wise Convolutions instead of Classical Convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the expected output using classical convolution is [4x4x5]\n",
      "The reduction factor is equal to 4.235294117647059\n"
     ]
    }
   ],
   "source": [
    "ho = 4\n",
    "wo = 4\n",
    "ci = 5\n",
    "nf = 8\n",
    "hf = 3\n",
    "wf = 3\n",
    "cf = 5\n",
    "\n",
    "def calcDepthWise(ho,wo,nf,hf,wf,ci):\n",
    "    MAC_DW = ho*wo*ci*(hf*wf+nf)\n",
    "    return 2*MAC_DW\n",
    "\n",
    "r_ops = calcClassical(ho,wo,nf,hf,wf,ci)/(calcDepthWise(ho,wo,nf,hf,wf,ci))\n",
    "\n",
    "print('The shape of the expected output using classical convolution is [4x4x5]')\n",
    "print('The reduction factor is equal to {0}'.format(r_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.1 - use following code to test\n",
    "\n",
    "Until which layer can we consider the network topology (with an input size of [ 224 x 224 x 3 ]) as a 'fully convolutional neural network', preserving spatial information from its input image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.2\n",
    "\n",
    "In the largest fully convolutional subset of MobileNet, by which factor do we \"downsample\" the input image's spatial dimensions? Compare the input dimensions to the network and the output dimensions of the FCN subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.3\n",
    "\n",
    "If we want to encode the input image to a spatial representation at 1/32 of both the input height and the input width (from an input dimensionality of [ 224 x 224 x 3 ]) , which layers can we select as an end point? Similarly, which layers can we choose as an endpoint if we want to \"downsample\" by a factor of 16? Modify my_endpoint .\n",
    "\n",
    "\n",
    "For a downsampling of 1/32, we can select the endpoint to be Conv2d_12_pointwise. For a downsampling of 1/16, we can select the endpoint to be Conv2d_11_pointwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2.4\n",
    "\n",
    "If you were to change the input size to e.g. [360 x 480 x 3], do we achieve the same downsampling factor for input height and the width if we set my_endpoint to 'Conv2d_13_pointwise' ?\n",
    "\n",
    "Since the input image size is not square, the downsampling factor for height (1/30) is less than the downsampling factor width (1/32). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.1.1\n",
    "\n",
    "Linking all of the knowledge above together, we will use a bilinear distribution as the weights initializer for the Transposed Convolution kernels of our FCN architecture. The initial weights are created in the following code block; inspect the implementation, modify nclasses to the number of classes from your use case, modify kernel_dims to your calculated value from Question 3.2.3, and then run the cell to visualize your initialized weights:\n",
    "\n",
    "#### Number of classes = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.2.1\n",
    "\n",
    "Now take a look at the following code block. Above layers' parameters can be adjusted in the function add_fcn_head() . To put this into the full context of our MobileNet architecture,\n",
    "mobilenet_fcn() creates the MobileNet stem we had derived earlier and attaches above FCN head to it. After studying the script below, fill in the number of classes you want to predict, and verify that you achieve a pixel-wise classification accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5.2.2\n",
    "\n",
    "What are the two regularization methods that are applied in the code defining the network? Type your answer in this Markdown cell.\n",
    "\n",
    "#### Regularization methods: dropout, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.1.1.\n",
    "\n",
    "First study, then modify the following code snippet to match your use case â€“ pay special attention to the number of classes you want to detect, as defined in your use case scenario. The output classes are predefined, however, you will have to figure out a mapping from the old classes to the 7 and 2 classes, respectively. Then run the script to crop all images to a resolution of 320 x 480 and perform the class re-mapping. The processed output files will be stored in /data/Cityscapes_preprocessed.\n",
    "\n",
    "mapping_list=[\n",
    "\n",
    "    ClassMapping(old_id=0, new_id=6, old_name='Unlabeled', new_name='Void'),\n",
    "    ClassMapping(old_id=1, new_id=6, old_name='Ego vehicle', new_name='Void'),\n",
    "    ClassMapping(old_id=2, new_id=1, old_name='Rectification border', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=3, new_id=6, old_name='Out of roi', new_name='Void'),\n",
    "    ClassMapping(old_id=4, new_id=1, old_name='Static', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=5, new_id=5, old_name='Dynamic', new_name='VRU'),\n",
    "    ClassMapping(old_id=6, new_id=6, old_name='Ground', new_name='Void'),\n",
    "    ClassMapping(old_id=7, new_id=2, old_name='Road', new_name='Road'),\n",
    "    ClassMapping(old_id=8, new_id=3, old_name='Sidewalk', new_name='Sidewalk'),\n",
    "    ClassMapping(old_id=9, new_id=4, old_name='Parking', new_name='Vehicles'),\n",
    "    ClassMapping(old_id=10, new_id=1, old_name='Rail track', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=11, new_id=1, old_name='Building', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=12, new_id=1, old_name='Wall', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=13, new_id=1, old_name='Fence', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=14, new_id=1, old_name='Guard rail', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=15, new_id=1, old_name='Bridge', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=16, new_id=1, old_name='Tunnel', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=17, new_id=1, old_name='Pole', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=18, new_id=1, old_name='Polegroup', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=19, new_id=1, old_name='Traffic light', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=20, new_id=1, old_name='Traffic sign', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=21, new_id=6, old_name='Vegetation', new_name='Void'),\n",
    "    ClassMapping(old_id=22, new_id=1, old_name='Terrain', new_name='Infrastructure'),\n",
    "    ClassMapping(old_id=23, new_id=0, old_name='Sky', new_name='Sky'),\n",
    "    ClassMapping(old_id=24, new_id=5, old_name='Person', new_name='VRU'),\n",
    "    ClassMapping(old_id=25, new_id=5, old_name='Rider', new_name='VRU'),\n",
    "    ClassMapping(old_id=26, new_id=4, old_name='Car', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=27, new_id=4, old_name='Truck', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=28, new_id=4, old_name='Bus', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=29, new_id=4, old_name='Caravan', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=30, new_id=4, old_name='Trailer', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=31, new_id=4, old_name='Train', new_name='Vehicle'),\n",
    "    ClassMapping(old_id=32, new_id=5, old_name='Motorcycle', new_name='VRU'),\n",
    "    ClassMapping(old_id=33, new_id=5, old_name='Bicycle', new_name='VRU'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.1.1\n",
    "\n",
    "Take pen and paper and estimate which parameter settings for DEPTH_MULTIPLIER , CLASSIFIER_KERNEL and CLASSIFIER_DEPTH are candidates for meeting the budget requirements of your use case. Hint: The spreadsheet indicates numbers in Mega (10^6) operations per single prediction while your budget is given in Giga (10^9) operations per second, and you will have to make use of the formula for calculating gigaops_per_second . It is recommended to use DEPTH_MULTIPLIER                                           ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.1.2\n",
    "\n",
    "Now verify your assumptions with below code block: From the previous question, you should know beforehand which DEPTH_MULTIPLIER is generally possible. Now you can play around with the other parameters, and also specify the number of cameras and the target frame rate from your use case. To get a better understanding about how the computations are distributed among the layers, we gave you a helper function printing out layer-wise statistics. If you have frozen your parameters, execute the code block and write down your computational needs in Giga operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
